{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tortch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-acabd0ec384a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mare\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0minitialize\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtortch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tortch'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "When we instantiate a model with from_pretrained(), \n",
    "the model configuration and pre-trained weights of the specified model \n",
    "are used to initialize the model.\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_batch=[\"I love messi\",\"I hate trump\"]\n",
    "encoding = tokenizer(text_batch,\n",
    "                     return_tensors='pt',\n",
    "                     padding = True,\n",
    "                     truncation= True\n",
    "                    )\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "labels = torch.tensor([1,0]).unsqueeze(0)\n",
    "# 这里的参数和实参名都是一样的\n",
    "# call a classification model with the labels argument,\n",
    "# the first returned element is the Cross Entropy loss between the predictions and the passed labels.\n",
    "outputs = model(input_ids,attention_mask=attention_mask,labels=labels)\n",
    "loss = outputs[0] \n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308277f6ef7a48cd921c36c8c21f0883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=434.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a0947ba3b14201a2f6f2950b8954aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1344997306.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification,Trainer,TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
    "training_args=TraingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    # number of warmup steps for learning rate scheduler\n",
    "    warmup_steps = 500,\n",
    "    weight_decay = 0.01, # strength of weight decay\n",
    "    logging_dir = './logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train() # to train\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# calculate additional metrics in addition to the loss\n",
    "def compute_metics(pred):\n",
    "    labels = pred.label_ids\n",
    "    pred = pred.predictions.argmax(-1)\n",
    "    precision,reacall,f1 ,_ = precision_recall_fscore_support(labels,preds,average='binary')\n",
    "    acc = accuracy_score(labels,preds)\n",
    "    return{\n",
    "        'accuracy':acc,\n",
    "        'f1':f1,\n",
    "        'precision':precision,\n",
    "        'recall':recall\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
