{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1184, 1132, 1128, 1833, 136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "{'input_ids': [101, 1184, 1132, 1128, 1833, 136, 102, 8499, 4286, 170, 3275, 1546, 1106, 3210, 1205, 189, 4847, 18290, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "{'input_ids': [[101, 1184, 1132, 1128, 1833, 136, 102, 0, 0, 0, 0, 0, 0], [101, 8499, 4286, 170, 3275, 1546, 1106, 3210, 1205, 189, 4847, 18290, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1.如何使用Transformers预处理数据？这个预处理工具我们就叫做 tokenizer。\n",
    "tokenizer  跟模型是一一对应的。\n",
    "tokenizer 将文本分割成tokens，然后将这些tokens 转换成数字，同时由其构成一个Tensor，\n",
    "最后作为输入数据输入到模型中。同时，为了让模型更好的工作，也有可能添加一些其它的输入到模型中。\n",
    "\n",
    "2.tokenizer()中参数的不同对结果造成的影响\n",
    "01.当tokenizer中传入单一句子？还是 pairs of sentence? 还是list？ \n",
    "02.tokenizer中传入的padding参数的含义\n",
    "\"\"\"\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "import torch\n",
    "text1= \"what are you doing?\"\n",
    "text2= \"Trump launch a executive order to shut down tiktok\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\"\"\"case1:传入单一句子\n",
    "\"\"\"\n",
    "res1 = tokenizer(text1)\n",
    "print(res1)\n",
    "\n",
    "\n",
    "\"\"\"case2:传入句子对（pairs of setuences ）\n",
    "01.这样应该是把 \"seq1,seq2\"拼成了一个序列，而不是两个序列。官方说这个是 pair of sentences，即句子对。\n",
    "这个句子对的用处有很多：判断句子对中的句子是否相似？ 问答系统的模型？\n",
    "02.注意这里的参数是text1,text2 而不是 [text1,text2]。 后者会在下面继续说道\n",
    "\"\"\"\n",
    "res2 = tokenizer(text1,text2)  \n",
    "print(\"\")\n",
    "print(res2)\n",
    "\n",
    "\n",
    "\"\"\"case3:传入列表\n",
    "传入列表的实际意义其实是多次”传入单一句子“的简写版\n",
    "\"\"\"\n",
    "res3 = tokenizer([text1,text2],padding=True) #这么看，tokenzier中就是两个序列，由一个list装了起来\n",
    "print(\"\")\n",
    "# a list of two sentences will be interpreted as a batch of two single sentences,\n",
    "print(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2769, 4263, 872, 102]\n",
      "<class 'list'>\n",
      "[CLS] 我 爱 你 [SEP]\n",
      "a= [101, 151, 8451, 8357, 102]\n",
      "b= {'input_ids': [101, 8873, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "c= [101, 8873, 102]\n",
      "d= {'input_ids': [101, 151, 8451, 8357, 102, 8873, 102], 'token_type_ids': [0, 0, 0, 0, 0, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "e= {'input_ids': [101, 100, 100, 100, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#使用encode\n",
    "tokenizer = BertTokenizer.from_pretrained('/Users/gamidev/program/resources/bert_base/')\n",
    "d = tokenizer.encode('我爱你')\n",
    "print(d) # 打印出转为向量的表示\n",
    "print(type(d)) # d是一个list，竟然不是一个tensor\n",
    "print(tokenizer.decode(d))  # 将向量解码成汉字\n",
    "\n",
    "# 测试一下encode_plus 的功能\n",
    "a = tokenizer.encode('I love you')\n",
    "print(\"a=\",a)\n",
    "b = tokenizer.encode_plus('China')\n",
    "print(\"b=\",b)\n",
    "c = tokenizer.encode('China')\n",
    "print(\"c=\",c)\n",
    "\n",
    "d = tokenizer.encode_plus('I love you','China')\n",
    "print(\"d=\",d) # 对这个attention_mask 不是很理解\n",
    "\n",
    "\n",
    "e = tokenizer.encode_plus(['I love you','China','what do you like'])\n",
    "print(\"e=\",e) # 对这个attention_mask 不是很理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
