{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel  # 这里的BertTokenizer 就是一个Tokenizer\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "# 但是下载得到的文件地址在哪里？\n",
    "# calling from_pretrained will fetch the model from the internet\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "# braking the text into tokens\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)\n",
    "# 需要注意的是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5000:5020]  # 运行结果是固定的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'after', 'stealing', 'money', 'from', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "       \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# print out the tokens\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\"\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "print(segments_ids)\n",
    "#但是疑问的是：这里分0/1的根据是“是不是一个整句吗？”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "convert inputs to PyTorch tensors\n",
    "原因是：BERT pytorch 接口要求输入的数据是tensor，而不是python 的list \n",
    "\"\"\"\n",
    "tokens_tensor = torch.tensor([indexed_tokens]) # 直接从一个python list 转换成 tensor \n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',  # fetch the model from the internet\n",
    "                                  output_hidden_states=True) # 模型是否返回所有的隐层\n",
    "\n",
    "# put the model in evaluation mode,\n",
    "# meaning feed-forward operation\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面的运行结果是：  => 但是我仍然不是很理解这是什么意思\n",
    "BertModel(\n",
    "  (embeddings): BertEmbeddings(\n",
    "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "    (position_embeddings): Embedding(512, 768)\n",
    "    (token_type_embeddings): Embedding(2, 768)\n",
    "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "    (dropout): Dropout(p=0.1, inplace=False)\n",
    "  )\n",
    "  (encoder): BertEncoder(\n",
    "    (layer): ModuleList(\n",
    "      (0): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (1): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (2): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (3): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (4): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (5): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (6): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (7): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (8): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (9): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (10): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "      (11): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (pooler): BertPooler(\n",
    "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "    (activation): Tanh()\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [ 0.2329,  0.1390,  0.2979,  ..., -0.0655,  0.8885,  0.5109],\n",
      "         [ 0.2257, -0.7165, -0.7255,  ...,  0.4844,  0.6030, -0.0957],\n",
      "         ...,\n",
      "         [-0.0374, -0.6155, -1.4419,  ...,  0.0793, -0.0811, -0.3802],\n",
      "         [-0.0228,  0.4207, -0.3288,  ...,  0.4464,  0.5178,  0.5501],\n",
      "         [-0.2350,  0.1566, -0.0462,  ..., -0.4206,  0.3074, -0.2288]]]), tensor([[[ 0.0522,  0.0595, -0.2179,  ...,  0.2280, -0.0712,  0.0148],\n",
      "         [ 0.3819,  0.1475,  0.2414,  ...,  0.3397,  0.7607,  0.4999],\n",
      "         [ 0.1705, -0.6168, -0.7296,  ...,  0.8631,  0.6274, -0.3727],\n",
      "         ...,\n",
      "         [ 0.6982, -0.4554, -1.7845,  ...,  0.3308,  0.0710, -0.5187],\n",
      "         [-0.0905,  0.1862, -0.4437,  ...,  0.2244,  0.1810,  0.3740],\n",
      "         [-0.0825,  0.0466, -0.1526,  ..., -0.2033,  0.3370, -0.1767]]]), tensor([[[-0.0357, -0.2022, -0.4103,  ...,  0.2511,  0.0586, -0.0547],\n",
      "         [ 0.1430,  0.0747,  0.0595,  ..., -0.2914,  0.1733,  0.4265],\n",
      "         [ 0.5752, -1.2385, -0.5650,  ...,  0.8995,  0.4652, -0.8080],\n",
      "         ...,\n",
      "         [ 0.9157, -0.4118, -1.6042,  ...,  0.1454,  0.1699, -0.2230],\n",
      "         [-0.1816,  0.0099,  0.0370,  ..., -0.2178,  0.0481,  0.3477],\n",
      "         [-0.1524, -0.1068, -0.0868,  ..., -0.1369,  0.2633, -0.3754]]]), tensor([[[-0.0183, -0.3853, -0.1600,  ...,  0.2446,  0.2160,  0.0633],\n",
      "         [ 0.7727, -0.3786,  0.6677,  ..., -0.2014,  0.0094,  0.6461],\n",
      "         [ 0.8822, -0.7807, -0.6305,  ...,  0.5223,  0.3687, -0.9875],\n",
      "         ...,\n",
      "         [ 0.9788, -0.0669, -1.6344,  ...,  0.0341,  0.0437, -0.1636],\n",
      "         [-0.4724, -0.0258,  0.3107,  ..., -0.2553, -0.1049,  0.0897],\n",
      "         [-0.0816, -0.1028,  0.0947,  ...,  0.0091,  0.0649, -0.0646]]]), tensor([[[ 0.0207, -0.3990, -0.8255,  ...,  0.3448,  0.0951,  0.3650],\n",
      "         [ 0.5667, -0.7513,  0.0695,  ..., -0.1052, -0.1700,  0.4333],\n",
      "         [ 0.7095, -0.2044, -0.1709,  ...,  0.7092,  0.0542, -1.0380],\n",
      "         ...,\n",
      "         [ 1.3519,  0.3352, -1.3555,  ...,  0.1676, -0.2817,  0.1042],\n",
      "         [-0.2488,  0.1044, -0.1778,  ..., -0.2713, -0.2643,  0.2310],\n",
      "         [-0.0266, -0.0469,  0.0053,  ...,  0.0085,  0.0417, -0.0486]]]), tensor([[[-0.3443, -0.6315, -0.6647,  ...,  0.0050,  0.1167,  0.3614],\n",
      "         [ 0.8173, -0.9999, -0.1752,  ..., -0.4932, -0.2556,  0.2038],\n",
      "         [ 0.8973, -0.2652, -0.6559,  ...,  0.5725,  0.4178, -0.6356],\n",
      "         ...,\n",
      "         [ 1.5582,  0.4479, -1.2054,  ...,  0.2052, -0.5083,  0.3994],\n",
      "         [-0.1898,  0.0898, -0.2766,  ..., -0.3044, -0.4370,  0.4248],\n",
      "         [-0.0264, -0.0364,  0.0304,  ...,  0.0160,  0.0067, -0.0486]]]), tensor([[[-2.3737e-01, -9.9507e-01, -4.5364e-01,  ..., -2.1456e-01,\n",
      "           3.5440e-01,  2.5447e-01],\n",
      "         [ 5.7495e-01, -9.1262e-01, -2.3175e-01,  ..., -1.8444e-01,\n",
      "          -1.8389e-01,  1.3615e-01],\n",
      "         [ 2.5019e-01, -5.6334e-01, -7.0052e-01,  ...,  2.0830e-01,\n",
      "           6.6902e-01,  5.5973e-02],\n",
      "         ...,\n",
      "         [ 1.5415e+00,  3.5884e-03, -1.0563e+00,  ...,  6.0116e-02,\n",
      "          -3.6085e-01,  5.3854e-01],\n",
      "         [-5.8238e-01,  8.6807e-02, -4.4414e-01,  ..., -8.1630e-01,\n",
      "          -4.2633e-01,  4.1039e-01],\n",
      "         [ 1.3974e-02, -5.6850e-02, -4.0642e-03,  ...,  7.8801e-05,\n",
      "          -1.4510e-02, -4.7457e-02]]]), tensor([[[-1.5213e-01, -9.7455e-01, -5.6574e-01,  ..., -6.0023e-01,\n",
      "           3.0606e-01,  3.6930e-01],\n",
      "         [ 2.7012e-01, -5.4540e-01,  1.6653e-02,  ...,  6.2675e-02,\n",
      "           2.6824e-01,  3.4129e-02],\n",
      "         [-4.1255e-01, -6.4567e-01, -2.0088e-01,  ...,  2.3458e-01,\n",
      "           1.4713e-01,  4.0926e-03],\n",
      "         ...,\n",
      "         [ 1.3217e+00, -6.6626e-02, -8.1355e-01,  ..., -3.9645e-01,\n",
      "          -5.2719e-01,  2.0277e-01],\n",
      "         [-5.6072e-01, -4.5253e-01, -5.8201e-01,  ..., -1.4335e+00,\n",
      "           2.8755e-01,  5.9470e-01],\n",
      "         [ 6.2760e-05, -2.2453e-02, -2.1868e-02,  ..., -1.4897e-02,\n",
      "           2.8921e-03, -4.3436e-02]]]), tensor([[[ 0.0967, -0.8567, -0.5056,  ..., -0.4204,  0.1267,  0.3225],\n",
      "         [-0.0109, -0.2976,  0.2986,  ..., -0.3344,  0.2755, -0.1571],\n",
      "         [-0.6171, -0.4909, -0.1630,  ..., -0.3340,  0.4452, -0.2641],\n",
      "         ...,\n",
      "         [ 0.9738,  0.0119, -0.6562,  ..., -0.3114, -0.1033,  0.1980],\n",
      "         [-0.8265, -0.4334, -0.7586,  ..., -1.2721,  0.0482,  0.3751],\n",
      "         [ 0.0196,  0.0017,  0.0323,  ..., -0.0348, -0.0410, -0.0743]]]), tensor([[[-0.3495, -0.7788, -0.7701,  ..., -0.0715,  0.2350,  0.1686],\n",
      "         [-0.2694, -0.3884, -0.3181,  ..., -0.7334,  0.0196, -0.4148],\n",
      "         [-0.5444, -0.3546, -0.2131,  ..., -0.5306,  0.2799, -0.3587],\n",
      "         ...,\n",
      "         [ 0.6356,  0.0980, -0.2775,  ..., -0.6243, -0.2620,  0.1343],\n",
      "         [-0.4167,  0.1132, -0.4761,  ..., -0.5415, -0.1268,  0.0483],\n",
      "         [-0.0671, -0.0451,  0.0232,  ..., -0.0718, -0.0020, -0.0556]]]), tensor([[[-0.9560, -1.0372, -0.8875,  ...,  0.0905, -0.3867,  0.3258],\n",
      "         [-0.3215, -1.1213, -0.2349,  ..., -0.5940,  0.1851, -0.5076],\n",
      "         [-0.4795, -1.0745, -0.1744,  ..., -0.5026,  0.1293, -0.1963],\n",
      "         ...,\n",
      "         [ 0.2736, -0.4021, -0.1525,  ..., -0.7099, -0.7498, -0.0066],\n",
      "         [-0.0143,  0.0187, -0.0490,  ..., -0.0197, -0.0281,  0.0165],\n",
      "         [-0.5765, -0.5547, -0.2122,  ...,  0.2610, -0.4111, -0.1223]]]), tensor([[[-0.6121, -0.6371, -0.8917,  ...,  0.1026, -0.2241,  0.3330],\n",
      "         [-0.2817, -0.6142, -0.4499,  ..., -0.7273,  0.0304, -0.5106],\n",
      "         [-0.4519, -0.2628, -0.1917,  ..., -0.5405, -0.2146, -0.2140],\n",
      "         ...,\n",
      "         [ 0.3213, -0.2997, -0.0471,  ..., -0.8094, -0.7861, -0.0618],\n",
      "         [ 0.0430,  0.0219, -0.0214,  ...,  0.0196, -0.0353,  0.0072],\n",
      "         [-0.0599, -0.6397, -0.6008,  ...,  0.4218, -0.5170, -0.1616]]]), tensor([[[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
      "         [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
      "         [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
      "         ...,\n",
      "         [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
      "         [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
      "         [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]]]))\n"
     ]
    }
   ],
   "source": [
    "# 我对下面这个hidden_states 的输出有些不理解\n",
    "\n",
    "# fetch the hidden states of the network\n",
    "# Run the text through BERT, and collect all of the hidden states produced from all 12 layers. \n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor,segments_tensors) #indexed_text => indexed_tokens => segments_tensor\n",
    "    hidden_states = outputs[2]\n",
    "print(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 22\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "# the word/token number in our sentence\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "一直不理解这里的想表示什么意思？\n",
    "\"\"\"\n",
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 5\n",
    "layer_i = 5\n",
    "print(batch_i) # batch[i] 值为0\n",
    "vec = hidden_states[layer_i][batch_i][token_i]#[2]\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of hidden_states: <class 'tuple'>\n",
      "Tensor shape for each layer:  torch.Size([1, 22, 768])\n"
     ]
    }
   ],
   "source": [
    "print('type of hidden_states:',type(hidden_states))\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 1, 22, 768])\n",
      "torch.Size([13, 22, 768])\n",
      "torch.Size([22, 13, 768])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = torch.stack(hidden_states,dim=0)\n",
    "print(token_embeddings.size())\n",
    "\n",
    "# 压缩第1个维度\n",
    "token_embeddings = torch.squeeze(token_embeddings,dim=1)\n",
    "print(token_embeddings.size())\n",
    "\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "print(token_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 3072\n"
     ]
    }
   ],
   "source": [
    "# Creating word and sentence vectors from hidden states\n",
    "\"\"\"现在既然已经得到了这些hidden states，那么该怎么操作？  主要目的有两个：\n",
    "01.得到每个tokens 的 vector\n",
    "02.得到整个句子的单个vector 表示\n",
    "\"\"\"\n",
    "token_vecs_cat = []\n",
    "for token in token_embeddings:\n",
    "    cat_vec = torch.cat((token[-1],token[-2],token[-3],token[-4]),dim=0)\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
