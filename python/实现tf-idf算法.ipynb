{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2410', '9755', '2281', '21686', '14501', '27155', '11937', '4018', '30335', '14485', '24583', '21092', '10449', '4898', '12868', '14360', '17281', '15', '469', '18455', '9755', '14547', '30335', '14485', '13274', '12052', '12443', '9866', '25682', '21357', '15431', '4054', '']\n",
      "['927', '1381', '17018', '1405', '1', '10937', '29842', '20791', '4940', '2281', '12235', '17605', '']\n",
      "['1116', '', '', '8612', '1116', '19551', '20866', '5697', '29504', '10937', '17281', '1193', '7', '4018', '11766', '17434', '10446', '8679', '16572', '17891', '4054', '', '']\n",
      "[[1 2 3]\n",
      " [5 6 7]]\n"
     ]
    }
   ],
   "source": [
    "#导入numpy库\n",
    "from numpy import *\n",
    "import re\n",
    "#解析文本数据函数\n",
    "#@delim 每一行不同特征数据之间的分隔方式，默认是tab键'\\t'\n",
    "def loadDataSet(filename,delim='\\t'):\n",
    "    #打开文本文件\n",
    "    with open(filename) as fr:\n",
    "    #对文本中每一行的特征分隔开来，存入列表中，作为列表的某一行\n",
    "    #行中的每一列对应各个分隔开的特征\n",
    "        for line in fr:            \n",
    "            line = line.strip(\"\\n\")\n",
    "            line= re.split(r'[，。 ]',line)\n",
    "            print(line)\n",
    "        # 将每行的值放到list中\n",
    "#         stringArr=[line.strip().split(delim) for line in fr.readlines()]\n",
    "#     print(stringArr)\n",
    "    #利用map()函数，将列表中每一行的数据值映射为float型\n",
    "#     ？datArr=[map(float.line) for line in stringArr]\n",
    "    datArr = [[1,2,3],[5,6,7]]\n",
    "    #将float型数据值的列表转化为矩阵返回\n",
    "    return mat(datArr)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    a = loadDataSet(\"open.txt\")\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['27155', '11937', '18455'], ['487', '22191', '2617'], ['19226', '15360', '27013'], ['24656', '29649', '18066'], ['17246', '29157', '8073'], ['4925', '24748', '2598'], ['29235', '22367', '21613'], ['16403', '5144', '7800'], ['22311', '24586', '9815'], ['20614', '15014', '21695']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "'''\n",
    "使用TF-IDF挑选出100个最有特征的词条，然后使用逻辑回归进行分类\n",
    "01.path 是大文本，里面包含多篇文档\n",
    "02.\n",
    "'''\n",
    "def read_doc(path):\n",
    "    DF = {} # DF{t}记录有多少篇文档包含单词t\n",
    "    TF = [] # TF{t,d} 记录所有的文档中单词出现的频次信息\n",
    "    word=() # 记录所有的word \n",
    "        \n",
    "    # 先计算出DF 的信息\n",
    "    with open(path,'r',encoding='utf-8') as f:\n",
    "        for line in f: \n",
    "            cur_TF = {}  # 记录当前doc中，单词t出现的频次\n",
    "            cur_words = [] # 记录当前这篇文档所有的单词\n",
    "#             print(line)\n",
    "            # 得到本doc 的所有文本\n",
    "            line = json.loads(line) #加载成dict\n",
    "            title = line['title']\n",
    "            content = line['content']\n",
    "            words = title + \" 。 \" + content\n",
    "            words = re.split('[，。 ！？]',words) # 得到本篇文档中所有的词\n",
    "            for word in words :\n",
    "                if word !=\" \" and word !=\"\":\n",
    "                    if word not in cur_TF.keys():\n",
    "                        cur_TF[word] = 1\n",
    "                    else:\n",
    "                        cur_TF[word] += 1\n",
    "                    cur_words.append(word)\n",
    "            TF.append(cur_TF) # 将信息放到TF中\n",
    "            \n",
    "            # 更新DF的值\n",
    "            for word in cur_words:\n",
    "                if word not in DF.keys():\n",
    "                    DF[word] = 1\n",
    "                else:\n",
    "                    DF[word] +=1\n",
    "    res = [] # 存储各个doc下的词特征\n",
    "#     print(\"DF=\",DF)\n",
    "    # 遍历所有的doc，依次找出top 100的TF-IDF 单词\n",
    "    for cur_TF in TF:\n",
    "#         print(cur_TF)\n",
    "        cur_val = {}\n",
    "        for item in cur_TF.items():\n",
    "            word,freq = item\n",
    "#             print(word,freq)\n",
    "            if word in DF.keys():                \n",
    "                val = freq / DF[word]\n",
    "                cur_val[word] = val # \n",
    "#                 print(\"val\",val)\n",
    "        cur_res = sorted(cur_val.items(),key=lambda x:x[1],reverse=True)\n",
    "        cur_words = []\n",
    "        top_k = 3 # 找出前100个词\n",
    "        for index in range(0,top_k):\n",
    "#             print(cur_res[index])\n",
    "            word,key = cur_res[index]\n",
    "            cur_words.append(word)\n",
    "            \n",
    "        res.append(cur_words)\n",
    "    print(res)\n",
    "read_doc(\"open.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
