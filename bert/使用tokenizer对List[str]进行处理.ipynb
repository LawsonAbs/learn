{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0911 12:23:44.920931 140080619095872 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /home/liushen/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.f12a4f986e43d8b328f5b067a641064d67b91597567a06c7b122d1ca7dfd9741\n",
      "I0911 12:23:44.922353 140080619095872 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0911 12:23:46.075798 140080619095872 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /home/liushen/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  9160,  2399,  2399,  2845,   704,  8024,  3419,  1213,  4510,\n",
       "          1690,  4917,  8024,  3418,  2945,   517,  3265,  6858,  4958,  6444,\n",
       "          6598,  6380,   518,  1355,  2357,  4638,  3144,  2945,  8024,  3419,\n",
       "          1213,   704,  1925,  4958,  6444,   809,  8122,   119,   128,   110,\n",
       "          4638,  2356,  1767,  1304,  3300,  4372,  4937,  1777,  6121,   689,\n",
       "          7987,  1928,  1765,   855,  8024,  2141,  4385,  1744,  1079,  2356,\n",
       "          1767,   100,  1061,  6825,  1094,   100,  8039,  5401,  4638,  7415,\n",
       "          1730,   738,  4917,  8024,  1062,  1385,   704,  1925,  4958,  6444,\n",
       "          1762,  5296,   677,  2356,  1767,   819,  7583,  3221,  8145,   110,\n",
       "          8024,  2961,  1399,  5018,   671,   102],\n",
       "        [  101,  2945,   749,  6237,  8024,   100,  3221,   671,  2157,  1762,\n",
       "          5739,  2247,  5335,  2209,   776,  5408,  2270,  3800,  1085,  2768,\n",
       "          4989,  4638,  1062,  1385,  8024,  4507,  3173,  3857,  5869,   752,\n",
       "          7270,  1076,   100,  3293,  1744,   836,  2971,  1169,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,   100,  1399,  1158,   831,  1501,  7415,  1730,  3221,  2408,\n",
       "          2336,  4761,  1399,  3696,  5852,   821,   689,  8024,  1062,  1385,\n",
       "          1762,  1059,  4413,  8430,   702,  1744,  2157,  1469,  1765,  1277,\n",
       "          2357,  2229,  7305,  2421,  6631, 12395,  2157,  8024,  1447,  2339,\n",
       "          6631,   123,   119,   125,   674,   782,  8024,  7415,  1730,  2809,\n",
       "          6121,  1199,  2600,  6161,  2476,  6612,  7509,  6432,  8024,  1358,\n",
       "          4554,  2658,  2512,  1510,  8024,  1062,  1385,  4909,  7032,   510,\n",
       "          2339,  6598,  5023,  3118,  1139,  1920,  8024,  6598,  7032,  1327,\n",
       "          1213,  1920,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,   100,  1399,  1158,   831,  1501,  7415,  1730,  3221,  2408,\n",
       "          2336,  4761,  1399,  3696,  5852,   821,   689,  8024,  1062,  1385,\n",
       "          1762,  1059,  4413,  8430,   702,  1744,  2157,  1469,  1765,  1277,\n",
       "          2357,  2229,  7305,  2421,  6631, 12395,  2157,  8024,  1447,  2339,\n",
       "          6631,   123,   119,   125,   674,   782,  8024,  7415,  1730,  2809,\n",
       "          6121,  1199,  2600,  6161,  2476,  6612,  7509,  6432,  8024,  1358,\n",
       "          4554,  2658,  2512,  1510,  8024,  1062,  1385,  4909,  7032,   510,\n",
       "          2339,  6598,  5023,  3118,  1139,  1920,  8024,  6598,  7032,  1327,\n",
       "          1213,  1920,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,   821,  3389,  3389,   928,  2622,  3227,  4850,  8024,   704,\n",
       "          7959,  3118,   802,  2898,  3300,  1920,  6825,  3209,  4403,  1062,\n",
       "          4500,  1305,  5500,   819,  3300,  7361,  1062,  1385,  8113,   110,\n",
       "          4638,  5500,   819,  8024,  2792,  2898,  5500,  3326,  2347,  6574,\n",
       "          2852,  5314,   704,  1744,  1045,  1920,  7213,  6121,  5500,   819,\n",
       "          3300,  7361,  1062,  1385,  1920,  6825,  5468,  1394,  6662,  3118,\n",
       "          6121,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"使用bert处理str list【意思是：list的每一项都是str】\n",
    "\"\"\"\n",
    "li = {'标签摘要': ['2019年年报中，格力电器称，根据《暖通空调资讯》发布的数据，格力中央空调以14.7%的市场占有率稳坐行业龙头地位，实现国内市场“八连冠”；美的集团也称，公司中央空调在线上市场份额是50%，排名第一', \n",
    "               '据了解，NewWaveMMXVLimited是一家在英属维尔京群岛注册成立的公司，由新浪董事长兼CEO曹国伟控制', \n",
    "               '”名创优品集团是广州知名民营企业，公司在全球93个国家和地区布局门店超4200家，员工超2.4万人，集团执行副总裁张赛音说，受疫情影响，公司租金、工资等支出大，资金压力大',\n",
    "               '”名创优品集团是广州知名民营企业，公司在全球93个国家和地区布局门店超4200家，员工超2.4万人，集团执行副总裁张赛音说，受疫情影响，公司租金、工资等支出大，资金压力大', \n",
    "               '企查查信息显示，中鼎支付持有大连明珠公用卡股份有限公司20%的股份，所持股权已质押给中国光大银行股份有限公司大连联合路支行'\n",
    "              ]}\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "# step1.处理li，得到一个List[str]\n",
    "li = li.get(\"标签摘要\") \n",
    "tokenizer(li, # 处理这个List\n",
    "          padding=True,# 保持一样的长度\n",
    "          max_length=5,\n",
    "          return_tensors=\"pt\" # 返回pytorch 用的tensor \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0911 14:55:05.134032 140658783070016 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/liushen/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "I0911 14:55:05.135207 140658783070016 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0911 14:55:15.149919 140658783070016 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/liushen/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 8667,  146,  112,  102],\n",
      "        [ 101, 1262, 1330, 5650,  102],\n",
      "        [ 101, 1262, 1103, 1304,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "batch_sentences = [\"Hello I'm a single sentence\",\n",
    "                    \"And another sentence\",\n",
    "                    \"And the very very last one\"]\n",
    "\n",
    "\"\"\"\n",
    "https://huggingface.co/transformers/preprocessing.html?highlight=tokenizer\n",
    "我之前一直认为padding的参数值只能是True，哪知道竟然还可以是别的值\n",
    "\"\"\"\n",
    "batch = tokenizer(batch_sentences,\n",
    "                  padding='max_length',  # 全部padding至最长的地方\n",
    "                  truncation=True,\n",
    "                  max_length=5,  \n",
    "                  return_tensors=\"pt\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 102], 'token_type_ids': [0, 0], 'attention_mask': [1, 1]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"能否将单句话padding? 可以padding至最大长度\n",
    "\"\"\"\n",
    "sen = \"I love you\"\n",
    "outp = tokenizer(sen,\n",
    "                 padding='max_length',\n",
    "                 max_length =2,\n",
    "                 truncation=True\n",
    "                )\n",
    "print(outp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
