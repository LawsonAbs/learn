{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"详细介绍tokenize方法\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sd', '##f']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1.tokenize()方法只接受字符串作为参数，如果传入一个字符串数组，则会报错\n",
    "2.如果该词在字典中为找到，那么就会被分割成多个字符串\n",
    "\"\"\"\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/lawson/pretrain/bert-base-uncased')\n",
    "print(tokenizer.tokenize(\"sdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they', 'threw', 'a', 'party', 'for', 'the', 'inventor', 'of', 'the', 'toast', '##er', '.', 'and', 'he', 'was', 'toast', '##ed', '.']\n",
      "['they']\n",
      "['threw']\n",
      "['a']\n",
      "['party']\n",
      "['for']\n",
      "['the']\n",
      "['inventor']\n",
      "['of']\n",
      "['the']\n",
      "['toast', '##er']\n",
      "['.']\n",
      "['and']\n",
      "['he']\n",
      "['was']\n",
      "['toast', '##ed']\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "texts = \"They threw a party for the inventor of the toaster . And he was toasted .\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/lawson/pretrain/bert-base-cased\")\n",
    "out = tokenizer.tokenize(texts)\n",
    "print(out)\n",
    "\n",
    "# 如果想找出某个单词被token 后的结果，那么就可以先用for 循环变量\n",
    "for word in texts.split():\n",
    "    temp = tokenizer.tokenize(word)\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ditch', 'dig', '##ger', ',', 'was', 'en', '##tre', '##nched', 'in', 'his', 'career', '.']\n",
      "['wa', '##l', '-', 'ma', '##rt', 'isn', \"'\", 't', 'the', 'only', 'saving', 'place', '!']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"一句话会被剔的干干净净。\n",
    "01.所有的符号会被单独弄出来\n",
    "02.isn't  => isn+'+t\n",
    "03.tokenize()方法只接收 文本作为参数\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/lawson/pretrain/bert-base-cased\")\n",
    "text = \"A ditch digger, was entrenched in his career.\"\n",
    "te2 = \"Wal-Mart isn't the only saving place!\"\n",
    "# 下面以 text 为例，介绍如何 使得 tokens 和 label一一对应起来\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token2 = tokenizer.tokenize(te2)\n",
    "print(tokens)\n",
    "print(token2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
