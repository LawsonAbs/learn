{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0928 16:04:55.330443 140231469553472 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/liushen/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0928 16:04:56.407803 140231469553472 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/liushen/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0928 16:04:56.408927 140231469553472 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0928 16:04:56.706316 140231469553472 modeling_utils.py:667] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/liushen/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0928 16:04:58.249141 140231469553472 modeling_utils.py:765] All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "I0928 16:04:58.249924 140231469553472 modeling_utils.py:774] All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "string = 'Hello, my dog is cute'\n",
    "print(len(string))\n",
    "inputs = tokenizer(string, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs[0]\n",
    "print(last_hidden_states.size()) #(1,8,768)  加了101,和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0928 16:12:44.841708 140231469553472 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/liushen/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0928 16:12:46.210260 140231469553472 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/liushen/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "I0928 16:12:46.211435 140231469553472 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0928 16:12:46.669749 140231469553472 modeling_utils.py:667] loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at /home/liushen/.cache/torch/transformers/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I0928 16:12:48.162254 140231469553472 modeling_utils.py:765] All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "I0928 16:12:48.163208 140231469553472 modeling_utils.py:774] All the weights of BertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "{'input_ids': tensor([[  101,  8667,   117,  1139,  3676,  1110, 10509,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]]), 'offset_mapping': tensor([[ 0,  0],\n",
      "        [ 0,  5],\n",
      "        [ 5,  6],\n",
      "        [ 7,  9],\n",
      "        [10, 13],\n",
      "        [14, 16],\n",
      "        [17, 21],\n",
      "        [ 0,  0]])}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'offset_mapping'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8fd0719d1284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(1,9,768)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'offset_mapping'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "string = 'Hello, my dog is cute'\n",
    "print(len(string))\n",
    "inputs = tokenizer(string, return_tensors=\"pt\",return_offsets_mapping=True)\n",
    "print(inputs)\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs[0]\n",
    "print(last_hidden_states.size()) #(1,9,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0928 17:01:08.076231 140231469553472 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /home/liushen/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "I0928 17:01:09.262284 140231469553472 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /home/liushen/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.f12a4f986e43d8b328f5b067a641064d67b91597567a06c7b122d1ca7dfd9741\n",
      "I0928 17:01:09.263553 140231469553472 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0928 17:01:09.552619 140231469553472 modeling_utils.py:667] loading weights file https://cdn.huggingface.co/bert-base-chinese-pytorch_model.bin from cache at /home/liushen/.cache/torch/transformers/a75f2e45a9463e784dfe8c1d9672440d5fc1b091d5ab104e3c2d82e90ab1b222.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "I0928 17:01:11.061474 140231469553472 modeling_utils.py:765] All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "I0928 17:01:11.062422 140231469553472 modeling_utils.py:774] All the weights of BertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "{'input_ids': tensor([[ 101, 2769, 3221, 1290,  691, 2360, 5745, 1920, 2110, 8439, 5277, 3173,\n",
      "         4495, 8024,  852, 3221, 3690, 3187, 3173, 7831, 2697,  511,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'offset_mapping': tensor([[ 0,  0],\n",
      "        [ 0,  1],\n",
      "        [ 1,  2],\n",
      "        [ 2,  3],\n",
      "        [ 3,  4],\n",
      "        [ 4,  5],\n",
      "        [ 5,  6],\n",
      "        [ 6,  7],\n",
      "        [ 7,  8],\n",
      "        [ 8, 12],\n",
      "        [12, 13],\n",
      "        [13, 14],\n",
      "        [14, 15],\n",
      "        [15, 16],\n",
      "        [16, 17],\n",
      "        [17, 18],\n",
      "        [18, 19],\n",
      "        [19, 20],\n",
      "        [20, 21],\n",
      "        [21, 22],\n",
      "        [22, 23],\n",
      "        [23, 24],\n",
      "        [ 0,  0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[[ 0,  0],101\\n[ 0,  1], 我\\n[ 1,  2],是\\n[ 2,  3],华\\n[ 3,  4],东\\n[ 4,  5],师\\n[ 5,  6],范\\n[ 6,  7],大\\n[ 7,  8],学\\n[ 8, 12],2020\\n[12, 13],级\\n[13, 14],新\\n[14, 15],生\\n...\\n[ 0,  0]])} 102\\n\\n01.有如下几个问题需要注意：原文的长度与text的长度不完全相关。有时，许多数字就会放在一起成为一个token，如上面的2020\\n这就导致我们在做序列标注的时候，会导致标注错误，因为按照字符标注的label与实际传入模型的token 的label是对不上的。\\n02.这种问题可能在其他情况里，也会出现，所以需要根据 return_offset_mapping 这个参数来进行修正\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = BertModel.from_pretrained('bert-base-chinese')\n",
    "string = '我是华东师范大学2020级新生，但是毫无 新鲜感。'\n",
    "print(len(string))\n",
    "inputs = tokenizer(string, return_tensors=\"pt\",return_offsets_mapping=True)\n",
    "print(inputs)\n",
    "'''\n",
    "[[ 0,  0],101\n",
    "[ 0,  1], 我\n",
    "[ 1,  2],是\n",
    "[ 2,  3],华\n",
    "[ 3,  4],东\n",
    "[ 4,  5],师\n",
    "[ 5,  6],范\n",
    "[ 6,  7],大\n",
    "[ 7,  8],学\n",
    "[ 8, 12],2020\n",
    "[12, 13],级\n",
    "[13, 14],新\n",
    "[14, 15],生\n",
    "[15, 16], ，\n",
    "[16, 17], 但\n",
    "[17, 18], 是\n",
    "[18, 19], 毫\n",
    "[19, 20], 无\n",
    "[20, 21], 新\n",
    "[21, 22], 鲜\n",
    "[22, 23], 感\n",
    "[23, 24], 。\n",
    "[ 0,  0]])} 102\n",
    "\n",
    "01.有如下几个问题需要注意：原文的长度与text的长度不完全相关。有时，许多数字就会放在一起成为一个token，如上面的2020\n",
    "这就导致我们在做序列标注的时候，会导致标注错误，因为按照字符标注的label与实际传入模型的token 的label是对不上的。\n",
    "所以需要做的就是，结合 return_offset_mapping 来生成标注。\n",
    "02.这种问题可能在其他情况里，也会出现，所以需要根据 return_offset_mapping 这个参数来进行修正。需要注意的是 return_offset_mapping 这个参数是需要通过\n",
    "BertTokenizerFast 这个模型来生成的，其它的不行。\n",
    "03.tokenizer 在解析汉语的过程中会把空格去掉。这就导致tokenized之后文本的总长度是要小于等于之前。我们可以通过下面这个反向解析看到其实空格不会被tokenized。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0928 21:58:13.159102 140231469553472 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /home/liushen/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2769, 1762, 2128, 2551, 7032, 2181, 7270, 1920, 511, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (6, 7), (7, 8), (9, 10), (10, 11), (11, 12), (0, 0)]}\n",
      "[101, 2769, 1762, 2128, 2551, 7032, 2181, 7270, 1920, 511, 102]\n",
      "['[CLS]', '我', '在', '安', '徽', '金', '寨', '长', '大', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"tokenizer 的正向与反向解析\n",
    "\"\"\"\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "string = \"我在安徽  金寨 长大。\"\n",
    "out = tokenizer(string,return_offsets_mapping=True)\n",
    "print(out)\n",
    "inputs_ids = out[\"input_ids\"]\n",
    "print(inputs_ids)\n",
    "print(tokenizer.convert_ids_to_tokens(inputs_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0929 15:17:41.204179 140231469553472 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /home/liushen/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "[CLS] 0\n",
      "清 1\n",
      "热 1\n",
      "， 0\n",
      "通 1\n",
      "淋 1\n",
      "。 0\n",
      "用 0\n",
      "于 0\n",
      "膀 0\n",
      "胱 0\n",
      "湿 0\n",
      "热 0\n",
      "， 0\n",
      "小 2\n",
      "便 2\n",
      "浑 2\n",
      "浊 2\n",
      "， 0\n",
      "淋 2\n",
      "沥 2\n",
      "作 2\n",
      "痛 2\n",
      "膀 0\n",
      "胱 0\n",
      "湿 0\n",
      "热 0\n",
      "通 0\n",
      "化 0\n",
      "振 0\n",
      "霖 0\n",
      "药 0\n",
      "业 0\n",
      "有 0\n",
      "限 0\n",
      "责 0\n",
      "任 0\n",
      "公 0\n",
      "司 0\n",
      "灯 0\n",
      "心 0\n",
      "草 0\n",
      "汤 0\n",
      "或 0\n",
      "温 0\n",
      "开 0\n",
      "水 0\n",
      "送 0\n",
      "服 0\n",
      "， 0\n",
      "一 0\n",
      "次 0\n",
      "6g 0\n",
      "， 0\n",
      "一 0\n",
      "日 0\n",
      "2～3 0\n",
      "次 0\n",
      "。 0\n",
      "6g 0\n",
      "* 0\n",
      "6 0\n",
      "袋 0\n",
      "* 0\n",
      "3 0\n",
      "盒 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "inputs_ids = [101, 3926, 4178, 8024, 6858, 3900, 511, 4500, 754, 5598, 5537, 3969, 4178, 8024, 2207, 912, 3847, 3843, 8024, 3900, 3769, 868, 4578, 5598, 5537, 3969, 4178, 6858, 1265, 2920, 7457, 5790, 689, 3300, 7361, 6569, 818, 1062, 1385, 4128, 2552, 5770, 3739, 2772, 3946, 2458, 3717, 6843, 3302, 8024, 671, 3613, 13147, 8024, 671, 3189, 10282, 3613, 511, 13147, 115, 127, 6150, 115, 124, 4665, 102,]   \n",
    "\n",
    "print(len(inputs_ids))\n",
    "out = tokenizer.convert_ids_to_tokens(inputs_ids)\n",
    "label = [0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "for a,b in zip(out,label):\n",
    "    print(a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
