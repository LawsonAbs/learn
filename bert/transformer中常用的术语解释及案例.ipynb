{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9468035101890564}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis') # å¦‚æœè¿è¡Œè¿™ä¸¤å¥å‘½ä»¤ï¼Œå°±ä¼šè‡ªåŠ¨ä¸‹è½½ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„æ¨¡å‹å’Œå¯¹å‘€çš„tokenizer\n",
    "classifier(\"what's your name?\")\n",
    "#classifier(\"We  want to show you the ğŸ¤— Transformers library.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 100, 8995, 11785, 9200, 8228, 8658, 8357, 8174, 100, 100, 11616, 119, 102, 100, 112, 161, 8880, 8617, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['æˆ‘', 'çˆ±', 'ä½ ']\n",
      "[2769, 4263, 872]\n",
      "[100, 100, 100]\n",
      "input_ids:[[101, 100, 8995, 11785, 9200, 8228, 8658, 8357, 8174, 100, 100, 11616, 119, 102], [101, 100, 10931, 8154, 8357, 9524, 112, 162, 11643, 8299, 8233, 119, 102, 0]]\n",
      "token_type_ids:[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask:[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "# åœ¨æœ¬ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨çš„Tokenizer å’Œ model \n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "\n",
    "# è¿™ä¸ªä¸‹è½½å†…å®¹åˆ°å“ªé‡Œäº†ï¼Ÿ å¯ä»¥åœ¨https://huggingface.co/transformers/installation.html#caching-models ä¸­\n",
    "# æŸ¥çœ‹æ­¤é—®é¢˜çš„è§£\n",
    "model_name = \"bert-base-chinese\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# returns a dictionary string to list of ints\n",
    "inputs = tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\",\"What's your name\")  # ç›´æ¥æŠŠè¿™å¥è¯è½¬æˆäº†toekens\n",
    "print(inputs)\n",
    "\n",
    "output = tokenizer.tokenize(\"æˆ‘çˆ±ä½ \")\n",
    "print(output)\n",
    "print(tokenizer.convert_tokens_to_ids(output))\n",
    "print(tokenizer.convert_tokens_to_ids(inputs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:[[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]\n",
      "attention_mask:[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n",
      "<class 'tuple'>\n",
      "tensor([[-4.0833,  4.3364],\n",
      "        [ 0.0818, -0.0418]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "the tokenizer is responsible for the preprocessing of your texts.\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pt_batch = tokenizer(\n",
    "    # pass a list of sentences directly to your tokenizer. ä½†æ˜¯æœ€é•¿ä¸è¶…è¿‡ä¸¤ä¸ªï¼Ÿ\n",
    "     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"], \n",
    "     padding=True,\n",
    "     truncation=True,\n",
    "     return_tensors=\"pt\"\n",
    " )\n",
    "\n",
    "# è¯´æ˜pt_batch æ˜¯ä¸€ä¸ªdict\n",
    "for key, value in pt_batch.items():\n",
    "    print(f\"{key}:{value.numpy().tolist()}\")\n",
    "pt_outputs = pt_model(**pt_batch)\n",
    "# all outputs are tuples (with only one element potentially)  => ä¹Ÿå¯èƒ½æ˜¯åªæœ‰ä¸€ä¸ªå…ƒç´ çš„å…ƒç»„\n",
    "# æ‰€æœ‰çš„Transformers æ¨¡å‹åé¦ˆçš„æ˜¯å°šæœªä½¿ç”¨æ¿€æ´»å‡½æ•°æ¿€æ´»çš„å€¼ï¼Œè¿™ä¹ˆåšçš„åŸå› æ˜¯ï¼šæˆ‘ä»¬é€šå¸¸ä¼šå°†æ¿€æ´»å‡½æ•°å’ŒæŸå¤±å‡½æ•°ä¸€èµ·ä½¿ç”¨ã€‚\n",
    "print(type(pt_outputs)) \n",
    "print(pt_outputs[0])# å› ä¸ºæ˜¯ä¸ªå…ƒç»„ï¼Œæ‰€ä»¥å–å…¶ä¸‹æ ‡ä¸º0çš„åœ°æ–¹\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 19\n",
      "input_ids : [[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]\n",
      "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ä»€ä¹ˆæ˜¯Attention mask?\n",
    "01.åœ¨ä½¿ç”¨å¥å­è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå„ä¸ªå¥å­å¾ˆéš¾ä¿è¯é•¿åº¦ç›¸åŒã€‚ä½†æ˜¯æˆ‘ä»¬åˆå¿…é¡»ç”¨è¿™äº›é•¿åº¦ä¸åŒçš„å¥å­è¿›è¡Œè®­ç»ƒï¼Œé‚£ä¹ˆè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ\n",
    "02.å¤„ç†çš„æ–¹æ³•æœ‰ä¸¤ä¸ªï¼šå…ˆæŠŠçŸ­çš„padding è‡³é•¿çš„é‚£ä¹ˆé•¿ï¼Œæˆ–è€…æ˜¯æŠŠé•¿çš„æˆªæˆçŸ­çš„é‚£ä¸ªé•¿åº¦ï¼Œè¿™æ ·å°±å¯ä»¥å¼€å§‹æ‰¹å¤„ç†ï¼Œè¿›è¡Œè®­ç»ƒã€‚\n",
    "03.ä½†æ˜¯ä¸€èˆ¬ä¸ºäº†ä¸æƒ³ä¸¢å¤±ä¿¡æ¯ï¼Œé€šå¸¸ä¼šä½¿ç”¨ç¬¬ä¸€ç§æ–¹æ³•ï¼Œå³å°†çŸ­çš„å˜é•¿ã€‚è¿™ä¸ªå˜é•¿å°±æ˜¯ä¿—ç§°çš„padding ã€‚\n",
    "é‚£ä¹ˆä¸æ˜¯ç»è¿‡paddingå¾—åˆ°çš„å€¼å¯¹åº”åœ¨attention_maskçš„ä½ç½®å°±æ˜¯1ï¼›ç›¸åï¼Œå¦‚æœæ˜¯ç»è¿‡ padding å¾—åˆ°çš„ï¼Œé‚£ä¹ˆè¯¥å€¼å¯¹åº”åœ¨\n",
    "attention_mask ä½ç½®ä¸Šçš„å€¼å°±æ˜¯0ã€‚\n",
    "04.å¯¹äºBertTokenizeræ¥è¯´ï¼Œ1å°±ä»£è¡¨åº”è¯¥è¢«â€œå…³ç…§ã€å³æ³¨æ„ã€‘â€çš„è¯ï¼Œè€Œ0å°±ä»£è¡¨æ˜¯ä¸€ä¸ªpadded valueã€‚\n",
    "\n",
    "05.ä¸‹é¢ç»™å‡ºä¸€ä¸ªå®é™…ä¾‹å­æ¥çœ‹ã€‚\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence_a = \"This is a short sequence.\"\n",
    "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "encoded_sequence_a = tokenizer(sequence_a)[\"input_ids\"]\n",
    "encoded_sequence_b = tokenizer(sequence_b)[\"input_ids\"]\n",
    "print(len(encoded_sequence_a),len(encoded_sequence_b))\n",
    "\n",
    "#æ³¨æ„è¿™é‡Œtokenizer()ä¸­çš„å‚æ•°æ˜¯ä¸€ä¸ªåˆ—è¡¨\n",
    "padded_sequences = tokenizer([sequence_a,sequence_b],padding=True)\n",
    "#print(padded_sequences)\n",
    "for key,value in padded_sequences.items():\n",
    "    print(key,\":\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Token Type IDs\n",
    "ä¸€äº›æ¨¡å‹çš„ç›®çš„æ˜¯åšå¥å­åˆ†ç±»æˆ–è€…æ˜¯é—®ç­”ç³»ç»Ÿã€‚è¿™å°±è¦æ±‚ä¸¤ä¸ªä¸åŒçš„å¥å­è¢«ç¼–ç æˆåŒä¸€ä¸ªinput IDsã€‚ ã€è¿™æ˜¯ä»€ä¹ˆé€»è¾‘ï¼Ÿï¼Ÿã€‘å®ƒä»¬é€šå¸¸ä½¿ç”¨ç‰¹æ®Šçš„æ ‡è®°æ¥åˆ†å‰²ï¼Œ\n",
    "è¯¸å¦‚classifier ä»¥åŠ åˆ†éš”ç¬¦ã€‚ä¾‹å¦‚ï¼šBERT æ¨¡å‹åœ¨å¤„ç†ä¸¤ä¸ªåºåˆ—çš„è¾“å…¥æ—¶ï¼š\n",
    "[....]\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sequence_a = \"HuggingFace is based in NYC\"\n",
    "sequence_b = \"Where is HuggingFace based?\"\n",
    "encoded_dict = tokenizer(sequence_a, sequence_b)\n",
    "decoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n",
    "\n",
    "print(decoded) \n",
    "# ä¸Šé¢çš„è¿™ä¸ªè¾“å‡ºå¯èƒ½å¯¹ä¸€èˆ¬çš„æ¨¡å‹å·²ç»è¶³å¤Ÿå¥½äº†ã€‚å› ä¸ºä¸€ä¸ªå¥å­çš„SEP å°±è¡¨ç¤ºç€ å¦ä¸€ä¸ªå¥å­çš„startã€‚ä½†æ˜¯BERTå´æœ‰å¦å¤–ä¸€ä¸ªæœºåˆ¶ â€”â€” token type IDs\n",
    "# åŒæ—¶ä¹Ÿè¢«å«åš segment IDsã€‚å®ƒä»¬ç”¨äºåŒºåˆ†æ¨¡å‹ä¸­ä¸åŒçš„åºåˆ—ã€‚\n",
    "print(encoded_dict['token_type_ids'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
