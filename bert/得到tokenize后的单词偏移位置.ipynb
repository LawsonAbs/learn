{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "01234567890123456\n",
    "What's your name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "{'input_ids': tensor([[ 101, 2054, 1005, 1055, 2115, 2171, 1029,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'offset_mapping': tensor([[[ 0,  0],\n",
      "         [ 0,  4],\n",
      "         [ 4,  5],\n",
      "         [ 5,  6],\n",
      "         [ 7, 11],\n",
      "         [12, 16],\n",
      "         [16, 17],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]]])}\n"
     ]
    }
   ],
   "source": [
    "'''必须使用继承自 PreTrainedTokenizerFast 的 Tokenizer，否则是无法得到 这个 return_offsets_mapping 参数\n",
    "1.offset 是左闭右开的区间\n",
    "'''\n",
    "from transformers import BertTokenizerFast\n",
    "text = \"What's your name?\"\n",
    "print(len(text))\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"/home/lawson/pretrain/bert-base-uncased/\")\n",
    "inputs = tokenizer(text,\n",
    "                  return_tensors=\"pt\",\n",
    "                  return_offsets_mapping= True,\n",
    "                  \n",
    "                   max_length = 50,\n",
    "                padding = \"max_length\",\n",
    "                  truncation=True\n",
    "                  )\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nihao': '2', '3': 'women'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\"\"\"\n",
    "将字符串以json的形式加载\n",
    "\"\"\"\n",
    "a= '{\"nihao\":\"2\",\"3\":\"women\"}'\n",
    "b = json.loads(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'input_ids': tensor([[  101,  2054,  1005,  1055,  2115,  2171,  1029,   102,     0,     0],\n",
      "        [  101,  2026,  2171,  2003, 14577,  1012,   102,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]), 'offset_mapping': tensor([[[ 0,  0],\n",
      "         [ 0,  4],\n",
      "         [ 4,  5],\n",
      "         [ 5,  6],\n",
      "         [ 7, 11],\n",
      "         [12, 16],\n",
      "         [16, 17],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]],\n",
      "\n",
      "        [[ 0,  0],\n",
      "         [ 0,  2],\n",
      "         [ 3,  7],\n",
      "         [ 8, 10],\n",
      "         [11, 17],\n",
      "         [17, 18],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0],\n",
      "         [ 0,  0]]])}\n",
      "<class 'torch.Tensor'>\n",
      "['[CLS]', 'what', \"'\", 's', 'your', 'name', '?', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "'''必须使用继承自 PreTrainedTokenizerFast 的 Tokenizer，否则是无法得到 这个 return_offsets_mapping 参数\n",
    "'''\n",
    "text = [\"What's your name?\",\"My name is lawson.\"]\n",
    "print(len(text))\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"/home/lawson/pretrain/bert-base-uncased/\")\n",
    "inputs = tokenizer(text,\n",
    "                  return_tensors=\"pt\",\n",
    "                   padding='max_length',\n",
    "                   max_length = 10,\n",
    "                  return_offsets_mapping= True)\n",
    "print(inputs)\n",
    "offset_mapping = inputs.get(\"offset_mapping\")\n",
    "print(type(offset_mapping))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3037, 7518]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset = offset_mapping.tolist() # 转成list\n",
    "input_ids = inputs.get(\"input_ids\") # 得到 input_ids\n",
    "#print(offset)\n",
    "word = ['interest','sweat']\n",
    "tokenizer.convert_tokens_to_ids(word) # 得到一串最后的下标"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
