{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0926 14:01:49.744884 139935223101248 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/liushen/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0926 14:01:49.746276 139935223101248 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0926 14:01:50.272951 139935223101248 modeling_utils.py:667] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/liushen/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0926 14:01:51.890630 139935223101248 modeling_utils.py:765] All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "I0926 14:01:51.891454 139935223101248 modeling_utils.py:774] All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "I0926 14:01:52.906155 139935223101248 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/liushen/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(inputs) <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "tensor([[[-4.3549e-02,  2.8564e-01,  1.4674e-02,  ..., -4.7990e-01,\n",
      "           3.4143e-01,  2.4718e-01],\n",
      "         [ 3.3014e-02,  4.0497e-02,  2.3604e-01,  ..., -6.2094e-02,\n",
      "           3.8769e-01,  1.5043e-01],\n",
      "         [-6.9909e-01,  9.9094e-01,  2.9854e-01,  ..., -1.1481e+00,\n",
      "          -1.2878e-03,  1.3819e-02],\n",
      "         [ 2.7440e-01,  9.2064e-01,  1.6139e+00,  ..., -6.3724e-01,\n",
      "          -3.4250e-01,  3.5173e-01],\n",
      "         [ 3.3919e-01,  8.8692e-01,  1.6468e-01,  ..., -7.4046e-01,\n",
      "          -2.4847e-01,  3.5895e-01],\n",
      "         [ 5.9084e-01,  1.6535e-01, -3.3053e-01,  ...,  1.3794e-02,\n",
      "          -5.7658e-01, -2.8880e-01]]])\n",
      "torch.Size([1, 6, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ninputs 是个字典，的内容如下：\\n{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), \\n'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), \\n'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\\n}\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "实现bert做embedding\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel # 导入transformer 的包\n",
    "#torch.set_printoptions(profile=\"full\") # 输出tensor的整个部分\n",
    "\"\"\"\n",
    "1.from_pretrained()方法是类PreTrainedModel的一个方法\n",
    "2.这里的如果加上 output_hidden_states=True，那么就会把所有的hidden_states 给输出\n",
    "如果没有加，那么就只能得到最后一个隐层的输出。\n",
    "\"\"\"\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\",\n",
    "                                    output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"hello,my dog\",return_tensors='pt')\n",
    "print('type(inputs)',type(inputs))\n",
    "#  执行model[BertModel实例]的forward()方法，但是在执行之前，仍然做了很多其他的事情\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    hiddden_states = outputs[2] # get the hidden states\n",
    "\n",
    "# The last hidden-state is the first element of the output tuple\n",
    "last_hidden_states = outputs[0]\n",
    "\n",
    "print(last_hidden_states)\n",
    "\n",
    "\"\"\"\n",
    "1.如果我的句子是 \"hello,my dog is cute\"，那么得到last_hidden_state 的size \n",
    "就是torch.Size([1, 8, 768])；如果我的句子是\"hello,my dog\"，那么得到的last_hidden_state\n",
    "的 就是 torch.size([1,6,768])。也就是中间那个维度的大小是跟句子长度有关系\n",
    " \n",
    "\"\"\"\n",
    "print(last_hidden_states.size())  #\n",
    "\n",
    "\"\"\"\n",
    "inputs 是个字典，的内容如下：\n",
    "{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), \n",
    "'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), \n",
    "'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0926 15:47:55.498247 139935223101248 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /home/liushen/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.f12a4f986e43d8b328f5b067a641064d67b91597567a06c7b122d1ca7dfd9741\n",
      "I0926 15:47:55.499525 139935223101248 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0926 15:47:55.784110 139935223101248 modeling_utils.py:667] loading weights file https://cdn.huggingface.co/bert-base-chinese-pytorch_model.bin from cache at /home/liushen/.cache/torch/transformers/a75f2e45a9463e784dfe8c1d9672440d5fc1b091d5ab104e3c2d82e90ab1b222.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "I0926 15:47:57.285943 139935223101248 modeling_utils.py:765] All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "I0926 15:47:57.286712 139935223101248 modeling_utils.py:774] All the weights of BertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "I0926 15:47:58.295547 139935223101248 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /home/liushen/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2769,  812, 6963, 3221,  704, 1744,  782,  102,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [ 101, 2769, 2218, 6438,  754, 1290,  691, 2360, 5745, 1920, 2110,  102,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])}\n",
      "\n",
      "torch.Size([2, 50])\n",
      "(tensor([[[-4.6232e-02,  6.4092e-01, -6.2718e-01,  ...,  8.9839e-01,\n",
      "           3.4320e-01, -1.7368e-01],\n",
      "         [ 3.1098e-01, -8.0042e-02, -4.6498e-01,  ..., -1.6565e-01,\n",
      "          -3.8034e-01, -8.5453e-02],\n",
      "         [ 4.5022e-01, -7.8874e-01,  3.2775e-01,  ...,  8.0031e-01,\n",
      "           7.5631e-01,  1.7191e-01],\n",
      "         ...,\n",
      "         [ 5.2140e-02, -2.6231e-04,  1.7568e-01,  ...,  4.5450e-01,\n",
      "          -2.9021e-01, -3.4960e-01],\n",
      "         [ 3.4861e-02,  1.7702e-01, -4.2946e-01,  ...,  4.4548e-01,\n",
      "          -2.6288e-01, -5.9067e-01],\n",
      "         [-1.7145e-01,  2.0429e-01, -7.1666e-01,  ...,  4.3113e-01,\n",
      "          -2.8166e-01, -3.8864e-01]],\n",
      "\n",
      "        [[ 3.0094e-01,  5.2804e-01,  1.1823e-01,  ..., -3.2782e-01,\n",
      "           1.2044e+00,  4.1891e-02],\n",
      "         [ 1.9049e-01,  1.3924e-01,  6.1395e-01,  ..., -1.0824e+00,\n",
      "           4.0179e-01, -3.5753e-02],\n",
      "         [-2.4365e-01, -1.2773e-01,  4.6215e-01,  ...,  1.3367e-01,\n",
      "           1.0624e+00,  4.1137e-01],\n",
      "         ...,\n",
      "         [-1.0611e+00,  1.9736e-01,  3.3355e-01,  ...,  3.9203e-02,\n",
      "           1.9023e-01, -4.2016e-01],\n",
      "         [-6.4521e-01,  1.2844e-01,  2.9018e-02,  ...,  2.0793e-01,\n",
      "           3.1902e-02, -3.7848e-01],\n",
      "         [-1.1315e+00,  2.9373e-01,  2.4874e-01,  ..., -9.2296e-04,\n",
      "           1.1562e-01, -3.9936e-01]]], grad_fn=<NativeLayerNormBackward>), tensor([[ 0.9996,  0.9999,  0.9946,  ..., -0.9986, -0.9877,  0.5874],\n",
      "        [ 0.9858,  0.9998,  0.9743,  ..., -0.9351, -0.9412,  0.9279]],\n",
      "       grad_fn=<TanhBackward>), (tensor([[[ 0.0588,  0.0704, -0.2139,  ..., -0.0237, -0.2234, -0.1116],\n",
      "         [ 0.4997, -0.2951,  1.0791,  ..., -0.9039,  0.2250, -0.4692],\n",
      "         [ 1.0978,  0.6032, -0.8269,  ..., -0.5443,  0.6507,  1.6306],\n",
      "         ...,\n",
      "         [ 1.1567, -0.1016, -0.5190,  ...,  1.9299,  0.1196,  0.2652],\n",
      "         [ 0.7012,  0.2227, -0.3654,  ...,  1.4255,  0.0943, -0.1744],\n",
      "         [ 0.6086, -0.0068, -0.2906,  ...,  2.0120,  0.1575,  0.3808]],\n",
      "\n",
      "        [[ 0.0588,  0.0704, -0.2139,  ..., -0.0237, -0.2234, -0.1116],\n",
      "         [ 0.4997, -0.2951,  1.0791,  ..., -0.9039,  0.2250, -0.4692],\n",
      "         [-0.1943,  0.3275, -0.2072,  ..., -0.7292, -0.4174, -0.2035],\n",
      "         ...,\n",
      "         [ 1.1567, -0.1016, -0.5190,  ...,  1.9299,  0.1196,  0.2652],\n",
      "         [ 0.7012,  0.2227, -0.3654,  ...,  1.4255,  0.0943, -0.1744],\n",
      "         [ 0.6086, -0.0068, -0.2906,  ...,  2.0120,  0.1575,  0.3808]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.0386, -0.0359, -0.2161,  ...,  0.0586, -0.0531, -0.0343],\n",
      "         [ 0.7802,  0.0171,  0.9153,  ..., -1.5213, -0.5238, -0.8598],\n",
      "         [ 1.2518,  0.4273, -0.9650,  ..., -0.6483,  0.3156,  1.8134],\n",
      "         ...,\n",
      "         [ 0.7558, -0.4952, -0.4373,  ...,  0.6103,  0.2623,  0.0347],\n",
      "         [ 0.4673, -0.2610, -0.3075,  ...,  0.3130,  0.2232, -0.2970],\n",
      "         [ 0.4086, -0.4074, -0.2829,  ...,  0.6388,  0.2798,  0.0398]],\n",
      "\n",
      "        [[-0.0049, -0.0275, -0.2159,  ...,  0.1222, -0.0348, -0.0106],\n",
      "         [ 0.2392, -0.3487,  1.3457,  ..., -0.8506, -0.2254, -0.6774],\n",
      "         [-0.1552,  0.4632, -0.3547,  ...,  0.2801, -0.5225, -0.2133],\n",
      "         ...,\n",
      "         [ 0.6580, -0.4045, -0.3608,  ...,  0.7553,  0.2943,  0.0022],\n",
      "         [ 0.3501, -0.1716, -0.2372,  ...,  0.4566,  0.2413, -0.3310],\n",
      "         [ 0.2775, -0.3078, -0.2343,  ...,  0.7864,  0.3037, -0.0097]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0137,  0.0177, -0.0880,  ...,  0.0058, -0.0619,  0.0272],\n",
      "         [ 0.3606,  0.1125,  1.2657,  ..., -1.1352, -0.3907, -0.7058],\n",
      "         [ 1.0393,  0.0328, -0.3640,  ..., -0.7248,  0.2662,  1.5879],\n",
      "         ...,\n",
      "         [ 0.6029, -0.5788,  0.0478,  ...,  0.2763,  0.2974,  0.0018],\n",
      "         [ 0.2491, -0.3425,  0.0846,  ...,  0.0483,  0.3219, -0.1973],\n",
      "         [ 0.2353, -0.4344,  0.0982,  ...,  0.2726,  0.4560,  0.1187]],\n",
      "\n",
      "        [[-0.0020,  0.0475, -0.0769,  ...,  0.0298, -0.0485,  0.0393],\n",
      "         [ 0.1986, -0.4267,  1.3787,  ..., -0.2113, -0.3575, -0.5616],\n",
      "         [-0.0405,  0.1406, -0.3300,  ...,  0.8842, -0.8144, -0.2691],\n",
      "         ...,\n",
      "         [ 0.5134, -0.5210,  0.1576,  ...,  0.5858,  0.3789, -0.0660],\n",
      "         [ 0.2150, -0.2830,  0.1398,  ...,  0.3290,  0.3745, -0.1956],\n",
      "         [ 0.2214, -0.3850,  0.1597,  ...,  0.5230,  0.4164,  0.1203]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.0312,  0.0892, -0.0065,  ...,  0.2200, -0.2614,  0.0854],\n",
      "         [ 0.8354, -0.2599,  0.8662,  ..., -0.5385, -0.4372, -0.4899],\n",
      "         [ 1.5221, -0.8093,  0.3048,  ..., -0.7610,  0.1928,  1.1293],\n",
      "         ...,\n",
      "         [ 0.6490, -0.5750,  0.1480,  ..., -0.1330,  0.2201,  0.5054],\n",
      "         [ 0.2235, -0.5186,  0.0189,  ..., -0.3301,  0.4320,  0.4543],\n",
      "         [ 0.2001, -0.3581,  0.0775,  ..., -0.1694,  0.7028,  0.5910]],\n",
      "\n",
      "        [[ 0.0306,  0.1083, -0.0051,  ...,  0.2105, -0.2306,  0.0170],\n",
      "         [ 0.4188, -0.5886,  1.4898,  ...,  0.2111, -0.2547, -0.8565],\n",
      "         [ 0.4348, -0.0031, -0.0901,  ...,  0.7884, -0.1007,  0.0489],\n",
      "         ...,\n",
      "         [ 0.5419, -0.5032, -0.1877,  ...,  0.0600,  0.4043,  0.2007],\n",
      "         [ 0.2253, -0.2975,  0.0085,  ..., -0.1389,  0.3603,  0.1742],\n",
      "         [ 0.2151, -0.3360,  0.0054,  ...,  0.0297,  0.5703,  0.4487]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.1218,  0.1101, -0.1141,  ...,  0.3773, -0.2765,  0.0368],\n",
      "         [ 1.0673, -0.4110,  0.4849,  ..., -0.4441,  0.0293, -0.1499],\n",
      "         [ 1.3423, -0.6433,  0.2348,  ..., -0.6957,  0.5941,  0.9812],\n",
      "         ...,\n",
      "         [ 0.7147, -0.4058,  0.0878,  ..., -0.0049,  0.2931,  0.2182],\n",
      "         [ 0.1852, -0.4992, -0.0249,  ..., -0.2041,  0.4004,  0.2215],\n",
      "         [ 0.2381, -0.2814, -0.0486,  ..., -0.1847,  0.6431,  0.3244]],\n",
      "\n",
      "        [[ 0.0728,  0.1102, -0.1000,  ...,  0.4213, -0.3115, -0.1075],\n",
      "         [ 0.6883, -0.6986,  1.4764,  ...,  0.1928,  0.2942, -0.6350],\n",
      "         [ 0.4119,  0.6303,  0.0750,  ...,  0.9587,  0.4929,  0.5285],\n",
      "         ...,\n",
      "         [ 0.5494, -0.6842,  0.0043,  ...,  0.7126,  0.3967,  0.0641],\n",
      "         [ 0.1588, -0.6358,  0.2929,  ...,  0.3519,  0.3121,  0.0860],\n",
      "         [ 0.1284, -0.6260,  0.1808,  ...,  0.3562,  0.5383,  0.3432]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0386,  0.3568, -0.4342,  ..., -0.0186, -0.1376, -0.0288],\n",
      "         [ 0.6216,  0.1954,  0.5646,  ..., -0.4726, -0.3014, -0.0037],\n",
      "         [ 0.9289, -0.0556,  0.1349,  ...,  0.1420,  0.2572,  1.0688],\n",
      "         ...,\n",
      "         [ 0.5361, -0.1602, -0.2043,  ..., -0.0037,  0.2703,  0.2807],\n",
      "         [ 0.1981, -0.3259, -0.2682,  ..., -0.2653,  0.3491,  0.2196],\n",
      "         [ 0.1988, -0.0734, -0.3810,  ..., -0.2430,  0.5754,  0.2687]],\n",
      "\n",
      "        [[ 0.1642,  0.1768, -0.3637,  ...,  0.2965, -0.2412, -0.0993],\n",
      "         [ 0.5195, -0.4955,  1.2458,  ...,  0.3439,  0.1431, -0.3085],\n",
      "         [-0.1073,  0.4029, -0.4146,  ...,  0.4058,  0.1302,  0.6498],\n",
      "         ...,\n",
      "         [ 0.5781, -0.6270, -0.4224,  ...,  0.0804,  0.4436, -0.0279],\n",
      "         [ 0.2951, -0.5570, -0.1102,  ..., -0.1659,  0.3975,  0.0974],\n",
      "         [ 0.3081, -0.4656, -0.2621,  ..., -0.1414,  0.6308,  0.2878]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.1737,  0.4643, -0.2764,  ...,  0.4722, -0.4210,  0.0051],\n",
      "         [ 0.1562,  0.5420,  0.4415,  ..., -0.2746, -0.2262, -0.7733],\n",
      "         [ 0.8494, -0.2355, -0.0595,  ...,  0.0101, -0.0164,  0.3931],\n",
      "         ...,\n",
      "         [ 0.5650, -0.1759, -0.2377,  ..., -0.0505,  0.1720, -0.2806],\n",
      "         [ 0.2655, -0.1205, -0.1805,  ..., -0.1603,  0.2837, -0.2419],\n",
      "         [ 0.2654,  0.1013, -0.4596,  ..., -0.1038,  0.6159, -0.1615]],\n",
      "\n",
      "        [[ 0.3404,  0.4616,  0.0102,  ...,  0.4710, -0.4599, -0.1734],\n",
      "         [ 0.4401, -0.7177,  1.2150,  ..., -0.0986, -0.0547, -0.3692],\n",
      "         [-0.0272, -0.1418, -0.0710,  ...,  0.4626,  0.2694,  0.1126],\n",
      "         ...,\n",
      "         [ 0.4909, -0.5092, -0.5913,  ..., -0.0826,  0.0710, -0.0851],\n",
      "         [ 0.2868, -0.4467, -0.1613,  ..., -0.2116, -0.0255, -0.0298],\n",
      "         [ 0.3562, -0.3942, -0.3208,  ..., -0.1429,  0.2757,  0.0819]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.7861,  0.3226, -0.0342,  ...,  0.1106, -0.7052,  0.2996],\n",
      "         [ 0.3629,  0.0803,  0.5241,  ..., -0.1593, -0.5131, -0.5866],\n",
      "         [ 0.5190, -0.6926, -0.0423,  ...,  0.3596, -0.4772,  0.7249],\n",
      "         ...,\n",
      "         [ 0.3974, -0.2910,  0.0646,  ..., -0.0785, -0.0318, -0.3775],\n",
      "         [ 0.2039,  0.0577, -0.1121,  ..., -0.2655,  0.1127, -0.7805],\n",
      "         [ 0.1362,  0.2854, -0.4559,  ..., -0.1659,  0.4867, -0.6344]],\n",
      "\n",
      "        [[ 0.0370,  0.2398, -0.0763,  ...,  0.1741, -0.4199,  0.1665],\n",
      "         [ 0.6324, -0.6186,  1.0687,  ...,  0.0292,  0.3372, -0.4653],\n",
      "         [-0.2173, -0.5490, -0.1310,  ...,  0.5005,  0.2987,  0.4273],\n",
      "         ...,\n",
      "         [ 0.0542, -0.5975, -0.4480,  ..., -0.1566,  0.2769, -0.2098],\n",
      "         [-0.0028, -0.4395, -0.3395,  ..., -0.4328,  0.0088, -0.1127],\n",
      "         [-0.1112, -0.5753, -0.3441,  ..., -0.3062,  0.3916, -0.2224]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-1.0218,  0.7267, -0.0731,  ...,  0.4163, -0.4886,  0.4365],\n",
      "         [-0.2970,  0.2657,  0.5756,  ..., -0.0988,  0.1027, -0.7068],\n",
      "         [ 0.0632, -0.8670,  0.7351,  ...,  0.6723,  0.1128,  0.5673],\n",
      "         ...,\n",
      "         [-0.0732,  0.0299,  0.2655,  ..., -0.0934,  0.1220, -0.3994],\n",
      "         [-0.4731,  0.2928, -0.2500,  ..., -0.1118, -0.2784, -0.8386],\n",
      "         [-0.4669,  0.2684, -0.5548,  ..., -0.0640, -0.0567, -0.7221]],\n",
      "\n",
      "        [[-0.1466,  0.3421, -0.2447,  ...,  0.2993, -0.0357,  0.4188],\n",
      "         [ 0.3842,  0.1817,  0.8413,  ..., -0.4233,  0.9222, -0.3646],\n",
      "         [ 0.0229,  0.1286, -0.3232,  ...,  0.5126,  0.1952,  0.2219],\n",
      "         ...,\n",
      "         [-0.2494, -0.3548, -0.1483,  ...,  0.1883, -0.0664, -0.3415],\n",
      "         [-0.2878, -0.1295,  0.0754,  ...,  0.0573, -0.3718, -0.2714],\n",
      "         [-0.5368, -0.2493,  0.1527,  ..., -0.0392, -0.0822, -0.4664]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-1.0589,  0.6386,  0.1286,  ...,  0.8948, -0.0983,  0.5060],\n",
      "         [-0.6162,  0.0873,  0.3671,  ...,  0.2729,  0.1186, -0.5433],\n",
      "         [-0.0205, -0.9553,  1.0125,  ...,  0.8046,  0.3324,  0.7205],\n",
      "         ...,\n",
      "         [-0.1427, -0.3570,  0.6306,  ...,  0.0150, -0.1544, -0.4678],\n",
      "         [-0.4499,  0.0534, -0.0125,  ...,  0.0569, -0.0563, -1.1926],\n",
      "         [-0.5850, -0.0372, -0.5493,  ...,  0.0955, -0.0872, -0.8886]],\n",
      "\n",
      "        [[ 0.1941,  0.0170, -0.1689,  ...,  0.5823,  0.9789,  0.0983],\n",
      "         [ 0.5513,  0.0319,  0.2592,  ..., -0.6946,  1.9032, -0.6730],\n",
      "         [ 0.0926,  0.0070, -0.6514,  ...,  0.3882,  0.8435, -0.2923],\n",
      "         ...,\n",
      "         [-0.4881, -0.0052, -0.7708,  ..., -0.1609,  0.5479, -1.4727],\n",
      "         [-0.2474,  0.0158, -0.7140,  ..., -0.1264,  0.3792, -1.0602],\n",
      "         [-0.6282,  0.0216, -0.5608,  ..., -0.2896,  0.2698, -1.4391]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0806,  0.6635, -0.3536,  ...,  0.8520,  0.1447, -0.5930],\n",
      "         [ 0.6277, -0.2453, -0.3201,  ...,  0.5234, -0.0342, -0.5953],\n",
      "         [ 0.8993, -0.6550,  0.3530,  ...,  0.8358,  0.0681, -0.2608],\n",
      "         ...,\n",
      "         [ 0.2443, -0.2005,  0.4408,  ..., -0.1028, -0.5090, -0.5838],\n",
      "         [ 0.0588,  0.4596, -0.1409,  ...,  0.2449, -0.4213, -1.0927],\n",
      "         [-0.0541,  0.2721, -0.6141,  ...,  0.1938, -0.5305, -0.6866]],\n",
      "\n",
      "        [[ 0.3126,  0.0016, -0.0209,  ...,  0.1007,  1.1923, -0.3426],\n",
      "         [ 0.8871, -0.5828,  0.4951,  ..., -0.7794,  1.4259, -0.8503],\n",
      "         [ 0.1607, -0.1017, -0.2475,  ...,  0.2656,  0.9460, -0.3301],\n",
      "         ...,\n",
      "         [-1.1179,  0.5292, -0.1086,  ..., -0.4225,  0.4888, -0.8597],\n",
      "         [-0.6263,  0.4284, -0.3201,  ..., -0.1823,  0.2772, -0.6728],\n",
      "         [-1.1026,  0.8139,  0.0370,  ..., -0.5529,  0.2551, -0.7582]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.1373,  0.7391, -0.1849,  ...,  0.6695,  0.2845, -0.9845],\n",
      "         [ 0.6151, -0.0825, -0.1802,  ..., -0.0125, -0.4090, -0.6908],\n",
      "         [ 0.6843, -0.3338,  0.4011,  ...,  0.3001,  0.2502, -0.2218],\n",
      "         ...,\n",
      "         [ 0.3053, -0.0370,  0.5121,  ..., -0.0348, -0.4850, -0.6944],\n",
      "         [-0.0119,  0.3746, -0.2161,  ...,  0.1067, -0.3675, -1.2994],\n",
      "         [-0.2861,  0.3420, -0.5921,  ..., -0.0316, -0.4393, -0.9476]],\n",
      "\n",
      "        [[ 0.3921,  0.3449,  0.2558,  ...,  0.2888,  1.0169, -0.3015],\n",
      "         [ 0.5042,  0.0689,  0.7584,  ..., -1.3695,  0.9198, -0.6810],\n",
      "         [ 0.2166, -0.0846,  0.0170,  ..., -0.0054,  0.8841, -0.0923],\n",
      "         ...,\n",
      "         [-1.1175,  0.5954,  0.6441,  ..., -0.2148,  0.5425, -0.4690],\n",
      "         [-0.6025,  0.4661,  0.2351,  ...,  0.0401,  0.3236, -0.5228],\n",
      "         [-1.2272,  0.8268,  0.6288,  ..., -0.2434,  0.5322, -0.3671]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-4.6232e-02,  6.4092e-01, -6.2718e-01,  ...,  8.9839e-01,\n",
      "           3.4320e-01, -1.7368e-01],\n",
      "         [ 3.1098e-01, -8.0042e-02, -4.6498e-01,  ..., -1.6565e-01,\n",
      "          -3.8034e-01, -8.5453e-02],\n",
      "         [ 4.5022e-01, -7.8874e-01,  3.2775e-01,  ...,  8.0031e-01,\n",
      "           7.5631e-01,  1.7191e-01],\n",
      "         ...,\n",
      "         [ 5.2140e-02, -2.6231e-04,  1.7568e-01,  ...,  4.5450e-01,\n",
      "          -2.9021e-01, -3.4960e-01],\n",
      "         [ 3.4861e-02,  1.7702e-01, -4.2946e-01,  ...,  4.4548e-01,\n",
      "          -2.6288e-01, -5.9067e-01],\n",
      "         [-1.7145e-01,  2.0429e-01, -7.1666e-01,  ...,  4.3113e-01,\n",
      "          -2.8166e-01, -3.8864e-01]],\n",
      "\n",
      "        [[ 3.0094e-01,  5.2804e-01,  1.1823e-01,  ..., -3.2782e-01,\n",
      "           1.2044e+00,  4.1891e-02],\n",
      "         [ 1.9049e-01,  1.3924e-01,  6.1395e-01,  ..., -1.0824e+00,\n",
      "           4.0179e-01, -3.5753e-02],\n",
      "         [-2.4365e-01, -1.2773e-01,  4.6215e-01,  ...,  1.3367e-01,\n",
      "           1.0624e+00,  4.1137e-01],\n",
      "         ...,\n",
      "         [-1.0611e+00,  1.9736e-01,  3.3355e-01,  ...,  3.9203e-02,\n",
      "           1.9023e-01, -4.2016e-01],\n",
      "         [-6.4521e-01,  1.2844e-01,  2.9018e-02,  ...,  2.0793e-01,\n",
      "           3.1902e-02, -3.7848e-01],\n",
      "         [-1.1315e+00,  2.9373e-01,  2.4874e-01,  ..., -9.2296e-04,\n",
      "           1.1562e-01, -3.9936e-01]]], grad_fn=<NativeLayerNormBackward>)))\n",
      "{'input_ids': [[101, 2769, 2110, 739, 156, 10986, 3175, 1403, 102], [101, 872, 4638, 4415, 2682, 3221, 784, 720, 8043, 102], [101, 976, 671, 1399, 4696, 6411, 3633, 4684, 831, 4899, 4638, 4906, 2825, 782, 1447, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n",
      "{'input_ids': [[101, 2769, 812, 6963, 3221, 704, 1744, 782, 102, 2769, 2110, 739, 156, 10986, 3175, 1403, 102], [101, 2769, 2218, 6438, 754, 1290, 691, 2360, 5745, 1920, 2110, 102, 872, 4638, 4415, 2682, 3221, 784, 720, 8043, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''下面这个程序详细介绍了tokenizer 中使用list 作为参数的用法\n",
    "'''\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel # 导入transformer 的包\n",
    "model = BertModel.from_pretrained(\"bert-base-chinese\",\n",
    "                                    output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "a = ['我们都是中国人','我就读于华东师范大学']\n",
    "b = ['我学习nlp方向','你的理想是什么？','做一名真诚正直优秀的科技人员']\n",
    "out1 = tokenizer(a,\n",
    "                 padding='max_length',\n",
    "               max_length = 50,\n",
    "               truncation=True,\n",
    "                return_tensors='pt')\n",
    "print(out1)\n",
    "print(\"\")\n",
    "print(out1['input_ids'].size())\n",
    "res1 = model(**out1)\n",
    "print(res1)\n",
    "\n",
    "out2 = tokenizer(b,                 \n",
    "               max_length = 512,\n",
    "               truncation=True)\n",
    "print(out2)\n",
    "print(\"\")\n",
    "\n",
    "out3 = tokenizer(a,b,\n",
    "               max_length = 512,\n",
    "               truncation=True)\n",
    "print(out3)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
