{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20, 768])\n",
      "tensor(-0.4046, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"下面比较使用bert直接处理和间接处理两种方式处理句子\n",
    "1.直接处理\n",
    "\"\"\"\n",
    "from transformers import BertTokenizer,BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/lawson/pretrain/bert-base-cased\")\n",
    "bert = BertModel.from_pretrained(\"/home/lawson/pretrain/bert-base-cased\")\n",
    "texts = \"They threw a party for the inventor of the toaster . And he was toasted .\"\n",
    "inputs = tokenizer(texts,return_tensors = 'pt')\n",
    "out = bert(**inputs)\n",
    "a,b = out\n",
    "print(a.size())\n",
    "print(a[0,1,1]) # 随便找个数输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'threw', 'a', 'party', 'for', 'the', 'inventor', 'of', 'the', 'toaster', '.', 'And', 'he', 'was', 'toasted', '.']\n",
      "18\n",
      "[1152, 3885, 170, 1710, 1111, 1103, 12989, 1104, 1103, 17458, 1200, 119, 1105, 1119, 1108, 17458, 1174, 119]\n",
      "20\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20, 768])\n",
      "tensor(-0.4046, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.间接处理。需要注意的事项：\n",
    "（1）attention_mask, input_ids, token_type_ids 都必须是二维的 \n",
    "（2）上述三者都必须是tensor\n",
    "\"\"\"\n",
    "temp = texts.split()\n",
    "print(temp)\n",
    "\n",
    "# 手动创建 inputs\n",
    "allTokens = []\n",
    "\n",
    "for word in temp:\n",
    "    tokens  = tokenizer.tokenize(word)\n",
    "    allTokens.extend(tokens)\n",
    "print(len(allTokens))\n",
    "\n",
    "input_ids = []\n",
    "for token in allTokens:\n",
    "    iid = tokenizer.convert_tokens_to_ids(token)\n",
    "    input_ids.append(iid)\n",
    "print(input_ids)\n",
    "input_ids.insert(0,101)\n",
    "input_ids.append(102)\n",
    "print(len(input_ids))\n",
    "\n",
    "\n",
    "import torch as t\n",
    "attention_mask = [1]*20\n",
    "token_type_ids = [0] * 20\n",
    "input_ids = t.tensor(input_ids)\n",
    "attention_mask = t.tensor(attention_mask)\n",
    "token_type_ids = t.tensor(token_type_ids)\n",
    "\n",
    "input_ids = input_ids.unsqueeze(0)\n",
    "attention_mask = attention_mask.unsqueeze(0)\n",
    "token_type_ids = token_type_ids.unsqueeze(0)\n",
    "print(input_ids.size())\n",
    "\n",
    "res = bert(input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            token_type_ids = token_type_ids)\n",
    "x = res[0]\n",
    "print(x.size())\n",
    "print(x[0,1,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
