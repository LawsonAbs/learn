{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1772, -0.5214,  0.9664], requires_grad=True)\n",
      "tensor([1., 0., 0.])\n",
      "tensor(0.8468, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "BCELoss()的输入是x,y。其中x,y 有着相同的参数设置。\n",
    "1. 如果是自己实现BCELoss() 损失，需要注意一点：当我们使用 log(y)时，当y->0时，可能会得到一个负无穷值，为了避免梯度爆炸，所以令 log(y->0) = -100\n",
    "'''\n",
    "import torchs\n",
    "import torch.nn as nn\n",
    "# step1. 定义需要用到的方法\n",
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss() # 只用于计算二元交叉熵损失函数\n",
    "\n",
    "# step2. 定义数据类型，input 和 target 都是一个一维的tensor\n",
    "input = torch.randn(3,requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "\n",
    "print(input)\n",
    "print(target) \n",
    "\n",
    "# step3.将数据先经过sigmoid处理，然后再和 target 进行计算损失\n",
    "output = loss(m(input),target) \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4637)\n",
      "tensor(0.1528)\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "a= t.tensor([9.9976e-01, 9.7448e-01, 9.8244e-01, 1.6868e-01, 9.5050e-01, 4.2414e-05])\n",
    "b = t.tensor([0.1, 0.81, 0.923, 0., 1., 0.])\n",
    "c = t.tensor([1, 0.91, 0.923, 0., 1., 0.])\n",
    "criterion = nn.BCELoss()\n",
    "loss = criterion(a,b)\n",
    "loss_2 = criterion(a,c)\n",
    "print(loss)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0913)\n",
      "tensor([-0.0913])\n",
      "tensor(0.2791)\n",
      "tensor([-0.2791])\n"
     ]
    }
   ],
   "source": [
    "# 下面这个示例说明BCELoss 不仅需要看标签为1的值，还需要计算标签为0 的值\n",
    "# 但其实我们知道 pred 到 c的距离是要比到 b 近的，但事实是 loss_ab < loss_ac。 所以在使用交叉熵的使用也是有讲究的\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "pred= t.tensor([9.1276e-01])\n",
    "\n",
    "b = t.tensor([1.0,])\n",
    "criterion = nn.BCELoss()\n",
    "loss_ab = criterion(pred,b)\n",
    "print(loss_ab)\n",
    "print(t.log(pred))\n",
    "\n",
    "c = t.tensor([0.92,])\n",
    "loss_ac = criterion(pred,c)\n",
    "print(loss_ac)\n",
    "print(c*t.log(pred) + (1-c) * t.log(1-pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
