{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0681,  0.2976, -0.9554, -0.8669, -0.1848,  1.4334, -0.2745,\n",
      "          -0.2192, -0.1230, -0.5360],\n",
      "         [ 1.0103, -1.3483,  1.8934,  0.0286,  0.6348,  0.0994,  1.3042,\n",
      "           0.3001,  0.7366,  0.1204],\n",
      "         [-1.6593,  0.5226,  0.3781, -0.9619, -1.0888, -0.3680,  1.8228,\n",
      "           1.2497, -0.1636,  0.6319],\n",
      "         [-0.3766,  1.4101,  0.7446, -0.7036, -0.1220,  2.4454, -1.9116,\n",
      "          -0.7363,  0.8857, -1.8490],\n",
      "         [-0.5305, -0.9742,  0.5887,  0.4808, -0.9718,  0.3426,  1.1690,\n",
      "          -1.0753,  1.3477,  0.7213]],\n",
      "\n",
      "        [[-0.3370, -0.4236, -2.3417,  0.1807, -0.3706, -0.2489, -0.3786,\n",
      "          -0.0561, -0.0498,  0.2140],\n",
      "         [-1.4466,  0.2551,  0.5458, -0.0162,  1.1776, -1.0256, -1.4984,\n",
      "          -0.7556,  0.0465, -0.0612],\n",
      "         [ 0.3875,  0.6042, -0.3165, -1.6586,  0.3647, -1.6719, -1.0361,\n",
      "          -1.2787,  1.6920, -1.2060],\n",
      "         [ 0.0542, -0.1122, -0.8964, -0.5361, -0.5247, -0.3065, -0.2570,\n",
      "          -0.8534, -0.8811, -0.5434],\n",
      "         [-0.3159, -0.1265, -0.0173, -1.7057,  0.4728, -0.2699, -1.4360,\n",
      "           0.1408, -1.5972, -0.1634]]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "本节讲解LayerNorm 的使用，均已NLP中的例子讲解。\n",
    "LayerNorm 中的 normalized_shape 参数表示在什么维度做均值和方差的计算。\n",
    "'''\n",
    "import torch as t\n",
    "# (batch_size, seq_len, emb_dim)\n",
    "a = t.randn(2,5,10)\n",
    "# elementwise_affine 表示是否有双仿射的转换参数，这里设置为False，是为了做下面的对比\n",
    "# 10 代表就是最后embedding维度做均值和方差，那么这样得到的均值和方差的大小 (2,5)。这样(a-Mean)/sqrt(Var) 就是\n",
    "# 最后LayerNorm 的值了\n",
    "ln = t.nn.LayerNorm(10,elementwise_affine=False)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3194,  0.6784, -1.2816, -1.1432, -0.0762,  2.4550, -0.2166,\n",
      "          -0.1300,  0.0205, -0.6256],\n",
      "         [ 0.6408, -2.1984,  1.7039, -0.5409,  0.1889, -0.4556,  0.9946,\n",
      "          -0.2141,  0.3113, -0.4304],\n",
      "         [-1.6383,  0.4698,  0.3302, -0.9645, -1.0871, -0.3907,  1.7260,\n",
      "           1.1723, -0.1932,  0.5754],\n",
      "         [-0.2680,  1.0799,  0.5778, -0.5147, -0.0760,  1.8609, -1.4261,\n",
      "          -0.5394,  0.6843, -1.3789],\n",
      "         [-0.7341, -1.2427,  0.5490,  0.4253, -1.2400,  0.2669,  1.2142,\n",
      "          -1.3587,  1.4191,  0.7010]],\n",
      "\n",
      "        [[ 0.0641, -0.0616, -2.8445,  0.8152,  0.0153,  0.1918,  0.0038,\n",
      "           0.4717,  0.4807,  0.8635],\n",
      "         [-1.4025,  0.6395,  0.9884,  0.3140,  1.7465, -0.8972, -1.4646,\n",
      "          -0.5733,  0.3892,  0.2599],\n",
      "         [ 0.7415,  0.9425,  0.0885, -1.1563,  0.7204, -1.1686, -0.5789,\n",
      "          -0.8040,  1.9515, -0.7365],\n",
      "         [ 1.7218,  1.1910, -1.3100, -0.1609, -0.1245,  0.5715,  0.7292,\n",
      "          -1.1728, -1.2612, -0.1841],\n",
      "         [ 0.2516,  0.5078,  0.6555, -1.6288,  1.3186,  0.3138, -1.2638,\n",
      "           0.8694, -1.4820,  0.4579]]])\n"
     ]
    }
   ],
   "source": [
    "out = ln(a)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1360],\n",
      "         [ 0.4779],\n",
      "         [ 0.0363],\n",
      "         [-0.0213],\n",
      "         [ 0.1098]],\n",
      "\n",
      "        [[-0.3812],\n",
      "         [-0.2779],\n",
      "         [-0.4120],\n",
      "         [-0.4856],\n",
      "         [-0.5018]]])\n",
      "tensor([[[0.4087],\n",
      "         [0.6901],\n",
      "         [1.0713],\n",
      "         [1.7570],\n",
      "         [0.7609]],\n",
      "\n",
      "        [[0.4750],\n",
      "         [0.6945],\n",
      "         [1.1623],\n",
      "         [0.0983],\n",
      "         [0.5463]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3194,  0.6784, -1.2816, -1.1432, -0.0762,  2.4550, -0.2166,\n",
       "          -0.1300,  0.0205, -0.6256],\n",
       "         [ 0.6408, -2.1984,  1.7039, -0.5409,  0.1889, -0.4557,  0.9946,\n",
       "          -0.2141,  0.3113, -0.4304],\n",
       "         [-1.6383,  0.4698,  0.3302, -0.9645, -1.0871, -0.3907,  1.7260,\n",
       "           1.1723, -0.1932,  0.5754],\n",
       "         [-0.2680,  1.0799,  0.5778, -0.5147, -0.0760,  1.8609, -1.4261,\n",
       "          -0.5394,  0.6843, -1.3789],\n",
       "         [-0.7341, -1.2427,  0.5490,  0.4253, -1.2400,  0.2669,  1.2142,\n",
       "          -1.3587,  1.4191,  0.7010]],\n",
       "\n",
       "        [[ 0.0641, -0.0616, -2.8445,  0.8152,  0.0153,  0.1918,  0.0038,\n",
       "           0.4717,  0.4807,  0.8635],\n",
       "         [-1.4025,  0.6396,  0.9884,  0.3140,  1.7465, -0.8973, -1.4646,\n",
       "          -0.5733,  0.3892,  0.2599],\n",
       "         [ 0.7415,  0.9425,  0.0885, -1.1563,  0.7204, -1.1686, -0.5789,\n",
       "          -0.8040,  1.9515, -0.7365],\n",
       "         [ 1.7219,  1.1911, -1.3100, -0.1609, -0.1246,  0.5715,  0.7292,\n",
       "          -1.1729, -1.2613, -0.1841],\n",
       "         [ 0.2516,  0.5078,  0.6555, -1.6288,  1.3186,  0.3138, -1.2638,\n",
       "           0.8694, -1.4820,  0.4579]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求出a的均值和方差。（维度是最后一个维度）\n",
    "m = a.mean(-1).unsqueeze(-1)\n",
    "var = t.var(a, dim=-1,unbiased=False).unsqueeze(-1)\n",
    "print(m)\n",
    "print(var)\n",
    "(a-m)/t.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 现在搞一个有参数的LayerNorm\n",
    "# 可以发现LayerNorma 中的参数 gamma 和 beta 是normalized_shape 维度的。为什么是这个维度？\n",
    "# 因为后面要用到 x-(Mean)/sqrt(var) * gamma + beta 这个，因为x是normalized_shape 维度的。\n",
    "b = t.randn(2,5,10)\n",
    "ln2 = t.nn.LayerNorm(10)\n",
    "out = ln2(b)\n",
    "\n",
    "# 可以看到这个参数是可以学习的\n",
    "for i in ln2.parameters():\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
