{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 18])\n",
      "tensor([[[ -2.8309, -10.0151,  -0.0609, -10.9483, -14.3901, -13.7052, -13.9295,\n",
      "          -13.7487, -13.9281, -13.0007, -15.1600, -14.6515, -14.5086, -13.4125,\n",
      "          -14.0148, -13.8961, -13.6852, -14.8683],\n",
      "         [ -0.1913,  -4.6355,  -5.3558,  -6.2308,  -6.7024,  -6.8400,  -7.1388,\n",
      "           -2.7930,  -7.3355,  -5.7388,  -6.9575,  -7.5986,  -6.8405,  -4.4837,\n",
      "           -5.3912,  -4.1761,  -2.9118,  -6.5754]]])\n",
      "tensor([[2, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "from torch.nn import LogSoftmax\n",
    "soft = LogSoftmax(dim=2)\n",
    "\n",
    "a = t.tensor([[[ 9.5059,  2.3217, 12.2759,  1.3885, -2.0533, -1.3684, -1.5927,\n",
    "          -1.4119, -1.5913, -0.6639, -2.8232, -2.3147, -2.1718, -1.0757,\n",
    "          -1.6780, -1.5593, -1.3484, -2.5315],\n",
    "         [ 4.9464,  0.5022, -0.2181, -1.0931, -1.5647, -1.7023, -2.0011,\n",
    "           2.3447, -2.1978, -0.6011, -1.8198, -2.4609, -1.7028,  0.6540,\n",
    "          -0.2535,  0.9616,  2.2259, -1.4377]]])\n",
    "print(a.size()) \n",
    "b = soft(a)\n",
    "print(b)\n",
    "c = b.argmax(2)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdfs我们都是\n",
      "{\"text\": \"网易是一家综合型的大型公司\", \"董事长\": \"丁磊\"}\n",
      "{\"歌曲\": \"七里香\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "a=[[{\"text\":\"网易是一家综合型的大型公司\",\"董事长\":\"丁磊\"}],[{\"歌曲\":\"七里香\"}]]\n",
    "file_path = './test.json'\n",
    "\n",
    "b = \"[sdfs我们都是\"\n",
    "b = b.strip(\"[\")\n",
    "print(b)\n",
    "with open(file_path,'w',encoding='utf-8') as f :\n",
    "    for val in a:\n",
    "        json_str = json.dumps(val,ensure_ascii=False)\n",
    "        json_str=json_str[1:-1]\n",
    "        print(json_str)        \n",
    "        f.write(json_str)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
      " 37%|███▋      | 3675/10000 [00:00<00:00, 36748.90it/s]\u001b[A\n",
      " 73%|███████▎  | 7306/10000 [00:00<00:00, 36615.23it/s]\u001b[A\n",
      "  2%|▏         | 217/10000 [00:30<11:33, 14.10it/s]it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(10000)):\n",
    "    for j in range(1000):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "x = torch.randint(4,(2,3),dtype=torch.float32)\n",
    "y = torch.ones((3,2), dtype = torch.float32)\n",
    "print(x)\n",
    "print(y)\n",
    "print(torch.matmul(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape # 最后得到的就是一个tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  6782,   122, 10340,   683,  6782,   792,  5305,  8170,  2399,\n",
      "          1062,  6228,   981,  1008,  1196,   517,  2608,  3215,   677,  4638,\n",
      "          1352,  7167,  4433,   518,  3221,  4384,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]]), 'offset_mapping': tensor([[[ 0,  0],\n",
      "         [ 0,  1],\n",
      "         [ 1,  2],\n",
      "         [ 2,  4],\n",
      "         [ 4,  5],\n",
      "         [ 5,  6],\n",
      "         [ 6,  7],\n",
      "         [ 7,  8],\n",
      "         [ 8, 12],\n",
      "         [12, 13],\n",
      "         [13, 14],\n",
      "         [14, 15],\n",
      "         [15, 16],\n",
      "         [16, 17],\n",
      "         [17, 18],\n",
      "         [18, 19],\n",
      "         [19, 20],\n",
      "         [20, 21],\n",
      "         [21, 22],\n",
      "         [22, 23],\n",
      "         [23, 24],\n",
      "         [24, 25],\n",
      "         [25, 26],\n",
      "         [26, 27],\n",
      "         [27, 28],\n",
      "         [28, 29],\n",
      "         [ 0,  0]]])}\n",
      "['[CLS]', '辑', '1', '##cd', '专', '辑', '介', '绍', '2009', '年', '公', '视', '偶', '像', '剧', '《', '恒', '星', '上', '的', '双', '钢', '琴', '》', '是', '环', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"/home/lawson/pretrain/bert-base-chinese\")\n",
    "text = \"辑1CD专辑介绍2009年公视偶像剧《恒星上的双钢琴》是环\"\n",
    "inputs = tokenizer(text,\n",
    "                  return_tensors=\"pt\",\n",
    "                  return_offsets_mapping= True)\n",
    "print(inputs)\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
