{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.datasets.translation.Multi30k'>\n",
      "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import Multi30k\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n",
    "\n",
    "print(vars(train_data.examples[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.datasets.translation.Multi30k'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.iterator.BucketIterator'>\n",
      "1\n",
      "\n",
      "[torchtext.data.batch.Batch of size 1]\n",
      "\t[.label]:[torch.LongTensor of size 1]\n",
      "\t[.text]:('[torch.LongTensor of size 1x50]', '[torch.LongTensor of size 1]')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "将一串英文转换成torchtext.datasets的格式\n",
    "\"\"\"\n",
    "import torch as t\n",
    "from torchtext.data import Field, Example, TabularDataset\n",
    "from torchtext.data import BucketIterator\n",
    "from torchtext import data\n",
    "import spacy\n",
    "import jieba\n",
    "\n",
    "# 构建分词标准\n",
    "def tokenize_zh(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "spacy_en = spacy.load('en')\n",
    "def tokenize_en(text):\n",
    "    # [::-1]  的含义就是：从下标-1开始，然后每次递增（步长是-1），直到最后\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)][::-1]\n",
    "\n",
    "# 构建Field\n",
    "TEXT = Field(sequential=True, tokenize=tokenize_en,\n",
    "                     use_vocab=True, batch_first=True,\n",
    "                     fix_length=50,\n",
    "                     eos_token=None, init_token=None,\n",
    "                     include_lengths=True, pad_token=0)\n",
    "\n",
    "LABEL = Field(sequential=False,\n",
    "              tokenize=tokenize_zh,\n",
    "              use_vocab=True,\n",
    "              batch_first=True)\n",
    "\n",
    "\n",
    "fields = [(\"label\", LABEL), (\"text\", TEXT)]\n",
    "train, valid = TabularDataset.splits(\n",
    "    path=\".\",\n",
    "    train=\"/home/lawson/program/wheels/seq2seq/data/test.tsv\",\n",
    "    validation=\"/home/lawson/program/wheels/seq2seq/data/test.tsv\",\n",
    "    format='tsv',\n",
    "    skip_header=False,\n",
    "    fields=fields)\n",
    "\n",
    "# 根据数据构建字典\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, val_iter = BucketIterator.splits((train, valid),\n",
    "                                             batch_sizes=(5,5),\n",
    "                                             device = t.device(\"cpu\"),\n",
    "                                             sort_key=lambda x: len(x.text), # field sorted by len\n",
    "                                             sort_within_batch=True,\n",
    "                                             repeat=False)\n",
    "print(type(train_iter))\n",
    "print(len(train_iter))\n",
    "for x in train_iter:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.dataset.TabularDataset'>\n",
      "{'label': 'I love you', 'text': ['我爱你']}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.dataset.TabularDataset'>\n",
      "{'label': 'I love you', 'text': ['我爱你']}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "将一串英文转换成torchtext.datasets的格式\n",
    "\"\"\"\n",
    "import torch as t\n",
    "from torchtext.data import Field, Example, TabularDataset\n",
    "from torchtext.data import BucketIterator\n",
    "from torchtext import data\n",
    "import spacy\n",
    "import jieba\n",
    "\n",
    "spacy_en = spacy.load('en') # 英文分词\n",
    "\n",
    "# 构建分词标准\n",
    "def tokenize_zh(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "def tokenize_en(text):\n",
    "    # [::-1]  的含义就是：从下标-1开始，然后每次递增（步长是-1），直到最后\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# 构建Field\n",
    "SRC = Field(sequential=True, tokenize=tokenize_en,\n",
    "                     use_vocab=True, batch_first=True,\n",
    "                     fix_length=50,\n",
    "                     eos_token=None, init_token=None,\n",
    "                     include_lengths=True, pad_token=0)\n",
    "\n",
    "TRG = Field(sequential=False,\n",
    "              tokenize=tokenize_zh,\n",
    "              use_vocab=True,\n",
    "              batch_first=True)\n",
    "\n",
    "fields = [(\"label\", SRC), (\"text\", TRG)]\n",
    "\n",
    "# 根据数据构建字典\n",
    "SRC.build_vocab(train)\n",
    "TRG.build_vocab(train)\n",
    "\n",
    "src_file = ['I love ShangHai.','What\\'s your name?'] # source\n",
    "trg_file = [\"我爱上海\",\"你叫什么？\"] # target\n",
    "\n",
    "examples=[]\n",
    "for src_line, trg_line in zip(src_file, trg_file):\n",
    "    src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "    if src_line != '' and trg_line != '':\n",
    "        temp = data.Example.fromlist([src_line, trg_line], fields)\n",
    "        examples.append(temp)\n",
    "\n",
    "print(type(train))\n",
    "print(vars(train.examples[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
